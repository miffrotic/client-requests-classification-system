{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb801382",
   "metadata": {},
   "source": [
    "## Ноут-шаблон под обучение моделей\n",
    "> **Важно: обязательно фиксируем рандом (`seed=42`) для всех библиотек и методов, где под капотом подразумевается рандом!** \n",
    "\n",
    "> **Почему это важно:** для гарантии консистентности данных при сравнении разных моделей и решений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbec7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# фиксирую то, что импортнул\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c14c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем необходимые ресурсы (выполнить один раз)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger_eng') <-- Новый пакет для POS-тегов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c4d45",
   "metadata": {},
   "source": [
    "### Импорт датасета и сплит на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974acd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>question about cancelling order {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've understood you have a question regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BQZ</td>\n",
       "      <td>i have a question about cancelling oorder {{Or...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've been informed that you have a question ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLQZ</td>\n",
       "      <td>i need help cancelling puchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I can sense that you're seeking assistance wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BL</td>\n",
       "      <td>I need to cancel purchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I understood that you need assistance with can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCELN</td>\n",
       "      <td>I cannot afford this order, cancel purchase {{...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I'm sensitive to the fact that you're facing f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flags                                        instruction category  \\\n",
       "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
       "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
       "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
       "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
       "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
       "\n",
       "         intent                                           response  \n",
       "0  cancel_order  I've understood you have a question regarding ...  \n",
       "1  cancel_order  I've been informed that you have a question ab...  \n",
       "2  cancel_order  I can sense that you're seeking assistance wit...  \n",
       "3  cancel_order  I understood that you need assistance with can...  \n",
       "4  cancel_order  I'm sensitive to the fact that you're facing f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем датасет\n",
    "df = pd.read_csv(\"customer_support_dataset_generated.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba340d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flags                                                        BLC\n",
       "instruction    I want to create an account and also see my de...\n",
       "category                                 ['ACCOUNT', 'DELIVERY']\n",
       "intent                    ['create_account', 'delivery_options']\n",
       "response       Certainly! To create a new account, please hea...\n",
       "Name: 28120, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[28120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffdc56",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbe5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обязательный блок: исправляем дублящиеся категории и интенты\n",
    "def process_category_unique(value):\n",
    "    if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "        try:\n",
    "            categories = ast.literal_eval(value)\n",
    "            unique_categories = list(set(categories))\n",
    "            # Если осталась одна категория - возвращаем как строку\n",
    "            if len(unique_categories) == 1:\n",
    "                return unique_categories[0]\n",
    "            else:\n",
    "                return ', '.join(sorted(unique_categories))\n",
    "        except:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "    \n",
    "    \n",
    "df['category'] = df['category'].apply(process_category_unique).astype(str)\n",
    "df['intent'] = df['intent'].apply(process_category_unique).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a9bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flags                                                        BLC\n",
       "instruction    I want to create an account and also see my de...\n",
       "category                                       ACCOUNT, DELIVERY\n",
       "intent                          create_account, delivery_options\n",
       "response       Certainly! To create a new account, please hea...\n",
       "Name: 28120, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[28120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c43bc4",
   "metadata": {},
   "source": [
    "**Дальше я делаю подготовку текстовок обращений**\n",
    "\n",
    "В целом, этот блок на ваше усмотрение можно кастомить, если я что-то забыл учесть, или у вас появилась классная идея, ~~или я где-то жестоко не прав и ошибся~~, но базовый минимум здесь - дропнуть английские стоп-слова и пунктуацию, привести слова к леммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd99a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, language='english', custom_stopwords=None):\n",
    "        \n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        \n",
    "        # Удаляем отрицания из стоп-слов, чтобы не терять смысл\n",
    "        negations = {'not', 'no', 'never', \"don't\", \"isn't\", \"wasn't\", \"couldn't\", \"aren't\"}\n",
    "        self.stop_words.difference_update(negations)\n",
    "        \n",
    "        # Для добавления кастомных стоп-слов\n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords)\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Шаблоны для удаления\n",
    "        self.patterns_to_remove = [\n",
    "            r'{{.*?}}', r'\\[.*?\\]', r'<.*?>', r'http\\S+', r'@\\w+', r'#\\w+'\n",
    "        ]\n",
    "    \n",
    "    def get_wordnet_pos(self, word):\n",
    "        \"\"\"\n",
    "        Функция определяет часть речи (POS-тег)\n",
    "        для более качественной лемматизации.\n",
    "        \"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN) # по умолчанию считаем существительным\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        for pattern in self.patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        \n",
    "        # Заменяем жесткое удаление на более гибкое.\n",
    "        # Удаляем все, что не является буквой, цифрой или пробелом.\n",
    "        # Это сохраняет номера заказов и артикулы.\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) > 1 and token not in self.stop_words:\n",
    "                \n",
    "                # Лемматизация теперь использует POS-теги (Part Of Speech)\n",
    "                pos_tag = self.get_wordnet_pos(token)\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos_tag)\n",
    "                \n",
    "                filtered_tokens.append(lemma)\n",
    "        \n",
    "        return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bdcb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor(language='english', custom_stopwords='fucking')\n",
    "\n",
    "df['clean_instruction'] = df['instruction'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66865be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'nor',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.stop_words # для понимания, что там лежит \n",
    "\n",
    "# no, not -> вероятно не нужно исключать, подумать\n",
    "# i do **not** want to cancel order, i want to change shipping address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db179d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>clean_instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>checking invoice</td>\n",
       "      <td>check invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15406</th>\n",
       "      <td>I call to download bill #00108</td>\n",
       "      <td>call download bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13070</th>\n",
       "      <td>help me see when will myarticle arrive</td>\n",
       "      <td>help see myarticle arrive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28738</th>\n",
       "      <td>Is there a delivery period for this item?</td>\n",
       "      <td>delivery period item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19525</th>\n",
       "      <td>can uhelp me earn several products</td>\n",
       "      <td>uhelp earn several product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33072</th>\n",
       "      <td>I need to switch to a different account.</td>\n",
       "      <td>need switch different account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26663</th>\n",
       "      <td>I have to know if there is anything wrong with...</td>\n",
       "      <td>know anything wrong rebate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20496</th>\n",
       "      <td>I have got to retrieve the PIN of my account</td>\n",
       "      <td>get retrieve pin account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>assistance to cancel purchase {{Order Number}}</td>\n",
       "      <td>assistance cancel purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33378</th>\n",
       "      <td>How do I sign up for a new service account?</td>\n",
       "      <td>sign new service account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17367</th>\n",
       "      <td>I cannot receive the fucking newsletter</td>\n",
       "      <td>not receive fuck newsletter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37535</th>\n",
       "      <td>I want to review the item I bought.</td>\n",
       "      <td>want review item bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>I need assistance to omdify my address</td>\n",
       "      <td>need assistance omdify address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25139</th>\n",
       "      <td>help me check the eta of purchase {{Order Numb...</td>\n",
       "      <td>help check eta purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>there are troubles correcting my address</td>\n",
       "      <td>trouble correct address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>i have got to see when will my parcel arrive c...</td>\n",
       "      <td>get see parcel arrive ya help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21082</th>\n",
       "      <td>i cannot notify of errors with sign-up</td>\n",
       "      <td>not notify error signup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>where to check what payment payment modalities...</td>\n",
       "      <td>check payment payment modality accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31016</th>\n",
       "      <td>I need to get a refund and also track my refund.</td>\n",
       "      <td>need get refund also track refund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20708</th>\n",
       "      <td>I do not know what I need tp do to recover my ...</td>\n",
       "      <td>not know need tp recover user pwd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39173</th>\n",
       "      <td>Delete. Account.</td>\n",
       "      <td>delete account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25593</th>\n",
       "      <td>I want help tosee the status of order {{Order ...</td>\n",
       "      <td>want help tosee status order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>I submitted a wrong address by mistake, can yo...</td>\n",
       "      <td>submit wrong address mistake edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>using platinum account</td>\n",
       "      <td>use platinum account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17476</th>\n",
       "      <td>assistance to ujnsubscribe to ur newsletter</td>\n",
       "      <td>assistance ujnsubscribe ur newsletter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "4630                                    checking invoice   \n",
       "15406                     I call to download bill #00108   \n",
       "13070             help me see when will myarticle arrive   \n",
       "28738          Is there a delivery period for this item?   \n",
       "19525                 can uhelp me earn several products   \n",
       "33072           I need to switch to a different account.   \n",
       "26663  I have to know if there is anything wrong with...   \n",
       "20496       I have got to retrieve the PIN of my account   \n",
       "816       assistance to cancel purchase {{Order Number}}   \n",
       "33378        How do I sign up for a new service account?   \n",
       "17367            I cannot receive the fucking newsletter   \n",
       "37535                I want to review the item I bought.   \n",
       "2650              I need assistance to omdify my address   \n",
       "25139  help me check the eta of purchase {{Order Numb...   \n",
       "2645            there are troubles correcting my address   \n",
       "13326  i have got to see when will my parcel arrive c...   \n",
       "21082             i cannot notify of errors with sign-up   \n",
       "5864   where to check what payment payment modalities...   \n",
       "31016   I need to get a refund and also track my refund.   \n",
       "20708  I do not know what I need tp do to recover my ...   \n",
       "39173                                   Delete. Account.   \n",
       "25593  I want help tosee the status of order {{Order ...   \n",
       "2767   I submitted a wrong address by mistake, can yo...   \n",
       "24328                             using platinum account   \n",
       "17476        assistance to ujnsubscribe to ur newsletter   \n",
       "\n",
       "                           clean_instruction  \n",
       "4630                           check invoice  \n",
       "15406                     call download bill  \n",
       "13070              help see myarticle arrive  \n",
       "28738                   delivery period item  \n",
       "19525             uhelp earn several product  \n",
       "33072          need switch different account  \n",
       "26663             know anything wrong rebate  \n",
       "20496               get retrieve pin account  \n",
       "816               assistance cancel purchase  \n",
       "33378               sign new service account  \n",
       "17367            not receive fuck newsletter  \n",
       "37535                want review item bought  \n",
       "2650          need assistance omdify address  \n",
       "25139                help check eta purchase  \n",
       "2645                 trouble correct address  \n",
       "13326          get see parcel arrive ya help  \n",
       "21082                not notify error signup  \n",
       "5864   check payment payment modality accept  \n",
       "31016      need get refund also track refund  \n",
       "20708      not know need tp recover user pwd  \n",
       "39173                         delete account  \n",
       "25593           want help tosee status order  \n",
       "2767       submit wrong address mistake edit  \n",
       "24328                   use platinum account  \n",
       "17476  assistance ujnsubscribe ur newsletter  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['instruction', 'clean_instruction']].sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536f850",
   "metadata": {},
   "source": [
    "### Сплит выборки\n",
    "Разбиваем на:\n",
    "- `train` - на нём обучаем модели\n",
    "- `validation` - на нём \"тюним\" модели, настраимаем гиперпараметры (когда GridSearch делаем, к примеру)\n",
    "- `test` - финальная выборка для оценки качества  Тестовый набор используется **ТОЛЬКО ОДИН РАЗ** в самом конце! При обучении и подборе параметров модель ничего не должна знать из этого датасета.\n",
    "\n",
    "Предлагаю бить в пропорции **70/15/15**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe8749",
   "metadata": {},
   "source": [
    "- train\n",
    "- validation для экспериментов\n",
    "- test откладываем и забываем - его доразбить для имитации сервиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39cf37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, \n",
    "                         y, \n",
    "                         val_size=0.15, \n",
    "                         test_size=0.15, \n",
    "                         random_state=42 # !!!\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Разделяет данные на train, validation и test наборы\n",
    "    \"\"\"\n",
    "\n",
    "    # Сначала отделяем test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "        # stratify=y  # есть классы всего лишь с 1 примером, поэтому просто шафлим\n",
    "    )\n",
    "    \n",
    "    # Затем разделяем на train и validation\n",
    "    relative_val_size = val_size / (1 - test_size)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, \n",
    "        y_train_val, \n",
    "        test_size=relative_val_size, \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "        # stratify=y_train_val  # есть классы всего лишь с 1 примером, поэтому просто шафлим\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Получаем выборки\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=df.drop(['intent'], axis=1), \n",
    "    y=df[['intent']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678c69f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40366, 6), (28256, 5), (6055, 5), (6055, 5))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6ae35",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "Пробуем следующие методы:\n",
    "- Bag of Words (мешок слов)\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "- FastText\n",
    "\n",
    "Статья, которой можно вдохновиться: https://habr.com/ru/articles/778048/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a002b8",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a20d2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00004587345current  00123842current  10  10101  101010  11111  11223  \\\n",
      "0                       0                0   0      0       0      0      0   \n",
      "1                       0                0   0      0       0      0      0   \n",
      "2                       0                0   0      0       0      0      0   \n",
      "3                       0                0   0      0       0      0      0   \n",
      "4                       0                0   0      0       0      0      0   \n",
      "...                   ...              ...  ..    ...     ...    ...    ...   \n",
      "28251                   0                0   0      0       0      0      0   \n",
      "28252                   0                0   0      0       0      0      0   \n",
      "28253                   0                0   0      0       0      0      0   \n",
      "28254                   0                0   0      0       0      0      0   \n",
      "28255                   0                0   0      0       0      0      0   \n",
      "\n",
      "       112233  112233445  113542617735902current  ...  yor  yoru  youe  \\\n",
      "0           0          0                       0  ...    0     0     0   \n",
      "1           0          0                       0  ...    0     0     0   \n",
      "2           0          0                       0  ...    0     0     0   \n",
      "3           0          0                       0  ...    0     0     0   \n",
      "4           0          0                       0  ...    0     0     0   \n",
      "...       ...        ...                     ...  ...  ...   ...   ...   \n",
      "28251       0          0                       0  ...    0     0     0   \n",
      "28252       0          0                       0  ...    0     0     0   \n",
      "28253       0          0                       0  ...    0     0     0   \n",
      "28254       0          0                       0  ...    0     0     0   \n",
      "28255       0          0                       0  ...    0     0     0   \n",
      "\n",
      "       youhave  youhelp  youir  youshow  ypur  yur  zelle  \n",
      "0            0        0      0        0     0    0      0  \n",
      "1            0        0      0        0     0    0      0  \n",
      "2            0        0      0        0     0    0      0  \n",
      "3            0        0      0        0     0    0      0  \n",
      "4            0        0      0        0     0    0      0  \n",
      "...        ...      ...    ...      ...   ...  ...    ...  \n",
      "28251        0        0      0        0     0    0      0  \n",
      "28252        0        0      0        0     0    0      0  \n",
      "28253        0        0      0        0     0    0      0  \n",
      "28254        0        0      0        0     0    0      0  \n",
      "28255        0        0      0        0     0    0      0  \n",
      "\n",
      "[28256 rows x 3277 columns]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "\n",
    "vec_bag = CountVectorizer()\n",
    "X_train_bag = vec_bag.fit_transform(X_train['clean_instruction'])\n",
    "X_val_bag = vec_bag.transform(X_val['clean_instruction']) \n",
    "\n",
    "# Результаты\n",
    "print(pd.DataFrame(X_train_bag.toarray(), columns=vec_bag.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae9613",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e65467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00004587345current  00123842current   10  10101  101010  11111  11223  \\\n",
      "0                     0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "1                     0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "2                     0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "3                     0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "4                     0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "...                   ...              ...  ...    ...     ...    ...    ...   \n",
      "28251                 0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "28252                 0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "28253                 0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "28254                 0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "28255                 0.0              0.0  0.0    0.0     0.0    0.0    0.0   \n",
      "\n",
      "       112233  112233445  113542617735902current  ...  yor  yoru  youe  \\\n",
      "0         0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "1         0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "2         0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "3         0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "4         0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "...       ...        ...                     ...  ...  ...   ...   ...   \n",
      "28251     0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "28252     0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "28253     0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "28254     0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "28255     0.0        0.0                     0.0  ...  0.0   0.0   0.0   \n",
      "\n",
      "       youhave  youhelp  youir  youshow  ypur  yur  zelle  \n",
      "0          0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "1          0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "2          0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "3          0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "4          0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "...        ...      ...    ...      ...   ...  ...    ...  \n",
      "28251      0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "28252      0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "28253      0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "28254      0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "28255      0.0      0.0    0.0      0.0   0.0  0.0    0.0  \n",
      "\n",
      "[28256 rows x 3277 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train['clean_instruction'])\n",
    "X_val_tfidf = vec_tfidf.transform(X_val['clean_instruction'])\n",
    "\n",
    "# Результаты\n",
    "print(pd.DataFrame(X_train_tfidf.toarray(), columns=vec_tfidf.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f378ff5",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b499391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743196, 1185370)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "X_train_tokenized_docs = [word_tokenize(doc) for doc in X_train['clean_instruction']]\n",
    "X_val_tokenized_docs = [word_tokenize(doc) for doc in X_val['clean_instruction']]\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=X_train_tokenized_docs,\n",
    "    vector_size=100,     # размер вектора\n",
    "    window=5,            # размер окна контекста\n",
    "    min_count=1,         # минимальная частота слова\n",
    "    workers=1,           # количество потоков, 1 - иначе рандомит\n",
    "    sg=1                 # 1 для skip-gram, 0 для CBOW\n",
    ")\n",
    "\n",
    "word2vec_model.train(X_train_tokenized_docs, total_examples=len(X_train_tokenized_docs), epochs=10)\n",
    "\n",
    "# взять предобученный w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06465359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n",
      "Размерность обучающей матрицы: (28256, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Скачиваем предобученную модель (Glove Twitter, размер вектора 100)\n",
    "pretrained_model = api.load(\"glove-twitter-100\")\n",
    "\n",
    "# Токенизация (превращаем предложения в списки слов)\n",
    "X_train_tokenized_docs = [word_tokenize(doc) for doc in X_train['clean_instruction']]\n",
    "X_val_tokenized_docs = [word_tokenize(doc) for doc in X_val['clean_instruction']]\n",
    "\n",
    "# Функция для усреднения векторов (немного адаптированная под предобученную модель)\n",
    "def document_vector(model, doc):\n",
    "    # Фильтруем слова: оставляем только те, что есть в словаре модели\n",
    "    # В предобученной модели мы обращаемся к словарю через model.key_to_index (или просто проверяем in model)\n",
    "    words = [word for word in doc if word in model.key_to_index]\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        # Если в предложении нет ни одного знакомого слова (или оно пустое),\n",
    "        # возвращаем вектор из нулей, чтобы код не упал с ошибкой\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # model[words] возвращает массив векторов для всех найденных слов.\n",
    "    # np.mean усредняет их в один вектор\n",
    "    return np.mean(model[words], axis=0)\n",
    "\n",
    "# 4. Применяем функцию ко всем документам\n",
    "X_train_w2v = np.array([document_vector(pretrained_model, doc) for doc in X_train_tokenized_docs])\n",
    "X_val_w2v = np.array([document_vector(pretrained_model, doc) for doc in X_val_tokenized_docs])\n",
    "\n",
    "print(f\"Размерность обучающей матрицы: {X_train_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e4744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор для слова 'help':\n",
      "[-2.97602229e-02  2.87921399e-01  2.08012775e-01  2.61957318e-01\n",
      " -2.13927090e-01 -7.44856298e-01  4.54741389e-01  1.29259288e-01\n",
      " -5.81863642e-01 -7.37532377e-02  4.36909497e-01 -1.42000780e-01\n",
      "  3.84372711e-01  3.71176720e-01  2.15179585e-02 -3.18511516e-01\n",
      "  1.67894587e-01 -4.42687720e-01 -3.07508737e-01 -9.98989224e-01\n",
      "  2.10191026e-01  1.51525095e-01  7.24495292e-01  3.41706306e-01\n",
      "  7.15186298e-01 -7.58386254e-01 -2.88622286e-02 -1.53265476e-01\n",
      "  5.16312383e-03 -6.02941573e-01  3.00668895e-01  9.05029029e-02\n",
      "  9.36316475e-02 -4.50585395e-01 -1.07777260e-01  3.85010988e-01\n",
      "  6.55538142e-01 -3.71116877e-01 -9.96763334e-02 -1.59316987e-01\n",
      " -2.55321175e-01 -5.00283957e-01  5.76937757e-02 -1.08016618e-01\n",
      " -2.20560171e-02  1.24073669e-01 -6.93714857e-01  3.58207494e-01\n",
      " -2.47616395e-01  6.71634376e-01  1.49149656e-01 -5.24880439e-02\n",
      " -9.14265573e-01  1.22338273e-01  2.42298841e-01 -1.14371881e-01\n",
      " -1.22924903e-02 -3.50149274e-01 -8.68295506e-02  3.42357039e-01\n",
      "  1.49076998e-01 -1.80886477e-01 -2.90385574e-01 -2.84686267e-01\n",
      " -1.60938188e-01  6.32008970e-01 -1.08684734e-01  2.81050324e-01\n",
      " -1.56965660e-04  1.13030754e-01  5.56736469e-01  1.26586711e+00\n",
      " -9.41151232e-02  3.11099533e-02  4.30945367e-01 -1.59978762e-01\n",
      "  3.98732990e-01  3.00195843e-01 -4.98124659e-02  3.22050720e-01\n",
      " -1.50400586e-02  5.61225079e-02 -2.58405775e-01  3.92372519e-01\n",
      "  7.31404573e-02 -1.97956890e-01 -2.80964021e-02  1.31045401e-01\n",
      "  1.95449695e-01  4.17290717e-01 -7.55954683e-02 -1.17508182e-02\n",
      "  1.69124603e-01  2.07224879e-02  6.70357645e-01  1.30207896e-01\n",
      "  2.15571061e-01 -4.29535434e-02  2.90844947e-01 -3.48687291e-01]\n",
      "\n",
      "Похожие слова на 'help':\n",
      "[('assistance', 0.8762202262878418), ('uhelp', 0.7630007266998291), ('fuck', 0.7177433967590332), ('goddamn', 0.6701585650444031), ('know', 0.6581145524978638), ('try', 0.6576940417289734), ('possible', 0.6547091007232666), ('asistance', 0.649557888507843), ('damn', 0.623344361782074), ('dont', 0.6197867393493652)]\n"
     ]
    }
   ],
   "source": [
    "# Получение векторов слов\n",
    "print(\"Вектор для слова 'help':\")\n",
    "print(word2vec_model.wv['help'])\n",
    "\n",
    "# Похожие слова\n",
    "print(\"\\nПохожие слова на 'help':\")\n",
    "print(word2vec_model.wv.most_similar('help', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fedb0ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор для слова 'help':\n",
      "[ 0.41755   0.68889  -0.46208   0.35115  -0.23694  -0.68691   1.1901\n",
      "  0.23397   0.37881  -0.23358   0.33652  -0.039475 -4.3266    0.62525\n",
      " -0.80124  -0.37718  -0.13806  -0.72706  -0.20413  -0.73376   0.14369\n",
      " -0.049832 -0.79746   0.55831  -0.20469   0.12483   0.19367   0.99597\n",
      "  0.38962   0.23743  -0.3175   -0.17651  -0.60194   0.52959   0.70239\n",
      " -0.40085   0.74437   0.08028   0.075221  0.58811  -0.55258   0.008635\n",
      "  0.15862  -0.34466   0.38218   0.2284   -0.37994  -0.041245 -0.51875\n",
      "  0.32053   0.12853  -0.59126   0.53532   0.010341  0.10907  -0.21717\n",
      " -0.59941   0.78575  -0.37995   0.2596   -0.18696   0.39263   0.50281\n",
      "  0.23801   0.7866    0.035248 -0.22407   0.067952 -0.46081   0.21476\n",
      " -0.012484  0.36379   0.51241   0.58936  -0.21477  -0.24591  -0.1853\n",
      " -0.47781  -0.4952   -0.57234   1.1957    0.41207   0.19577  -0.5065\n",
      "  0.19058  -0.33058  -0.74558  -0.40009   0.25671   0.54588   0.084593\n",
      " -0.42773   0.43142  -0.25585   0.059181 -0.45639  -0.1141    0.37671\n",
      "  0.41033   0.39047 ]\n",
      "\n",
      "Похожие слова на 'help':\n",
      "[('need', 0.8208698630332947), ('please', 0.8192464709281921), ('helping', 0.8043481111526489), ('us', 0.7762775421142578), ('must', 0.7691217064857483), ('let', 0.7680193781852722), ('pls', 0.7564061284065247), ('give', 0.7560991644859314), ('can', 0.7538463473320007), ('trying', 0.75226891040802)]\n"
     ]
    }
   ],
   "source": [
    "# Получение векторов слов\n",
    "print(\"Вектор для слова 'help':\")\n",
    "print(pretrained_model['help'])\n",
    "\n",
    "# Похожие слова\n",
    "print(\"\\nПохожие слова на 'help':\")\n",
    "print(pretrained_model.most_similar('help', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba9db8",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e74c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743480, 1185370)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "fasttext_model.build_vocab(X_train_tokenized_docs)\n",
    "fasttext_model.train(\n",
    "    X_train_tokenized_docs, \n",
    "    total_examples=len(X_train_tokenized_docs), \n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "228f402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор для существующего слова 'help':\n",
      "[-0.280823   -0.52019906  0.04496343  0.10337804 -0.23796153  0.0341261\n",
      " -0.19240457  0.5989376   0.3635937  -0.9477554   0.09990043 -0.35251018\n",
      " -0.10727239  0.28350145  0.5472687  -0.7253978   0.244883    0.07058468\n",
      " -0.3493659  -0.50163466  0.17107655 -0.08316001  0.27093768  0.15646604\n",
      " -0.06674644 -0.64147496  0.02320246  0.16206717 -0.5552184   0.14339775\n",
      " -0.82274586  0.17867315  0.77332747  0.13780138 -0.23408133 -0.15446204\n",
      " -0.535891   -0.6433006   0.43265593  0.6328216  -0.45555556 -0.32184964\n",
      " -0.2102307  -0.32122886 -0.47794172 -0.17973186  0.31489056  0.11421549\n",
      "  0.04988303  0.4028192   0.63853204 -0.5740908   0.20030802 -0.02928838\n",
      "  0.18040204 -0.05406512  0.05711975  0.02496478  0.5413362   0.2841979\n",
      "  0.15843563  0.04333622 -0.17841671  0.06021926 -0.2715158   0.31578878\n",
      "  0.24584891 -0.27004483  0.3954763   0.16044621 -0.05408617 -0.08031064\n",
      "  0.24039108 -0.31412217  0.02891675 -0.19632743  0.35999453  0.8099027\n",
      " -0.09918266  0.12846942  0.06917363 -0.51967466  0.36607188 -0.1143256\n",
      " -0.15972508 -0.39467105 -0.2209093   0.42048952  0.36440346 -0.49855652\n",
      " -0.56278616 -0.47361335 -0.0147678  -0.07956412 -0.0744407   0.21304344\n",
      " -0.5780249  -0.7051363  -0.04378503  0.24645068]\n",
      "\n",
      "Вектор для OOV слова 'looooooooooooooool':\n",
      "[-0.04001096  0.02473453  0.02792316 -0.00039779  0.00069037 -0.0024172\n",
      " -0.00696924  0.02942461  0.02575206 -0.06182649 -0.08822065 -0.03211689\n",
      "  0.0054133   0.0427498   0.01236614 -0.06339419 -0.02708909 -0.03026738\n",
      " -0.01198606 -0.06375237 -0.02087586 -0.01017589 -0.02844136 -0.02303299\n",
      " -0.04785995 -0.02181242  0.01586431  0.00873839  0.04419072  0.01388328\n",
      " -0.01931903  0.04402959  0.05220904  0.02573192 -0.02523097  0.01122915\n",
      " -0.04860048  0.03328967 -0.04450236  0.03857386  0.01047911 -0.0157369\n",
      " -0.00509276 -0.08098833 -0.04531572 -0.03485595  0.01877283 -0.0235539\n",
      " -0.01249865  0.00205794  0.0711528  -0.00935174  0.01827964  0.05152682\n",
      "  0.0307041  -0.00621909  0.00409147  0.00717749 -0.01369048 -0.04459986\n",
      "  0.03243534 -0.05161656 -0.01817997  0.03704436 -0.0004903   0.05896242\n",
      "  0.02279377 -0.00939479  0.02246043 -0.0067015  -0.02863775  0.01796034\n",
      " -0.03695188 -0.01239471 -0.02042064 -0.00150175  0.0428094   0.00918564\n",
      " -0.00486026  0.01195351 -0.00083163 -0.01687611  0.04607745 -0.01506723\n",
      " -0.00577606 -0.02744507 -0.05811939  0.03794961  0.00375103 -0.04381169\n",
      " -0.0189539  -0.01651476 -0.02238082 -0.0013236   0.03560531  0.04490537\n",
      " -0.0204444  -0.05887433  0.04199827  0.00072949]\n",
      "\n",
      "Похожие слова на 'help':\n",
      "[('yahelp', 0.9452125430107117), ('uelp', 0.9356333613395691), ('helpp', 0.9277247190475464), ('youhelp', 0.9267723560333252), ('assistance', 0.925699770450592), ('helpo', 0.9251793622970581), ('assistancee', 0.9222808480262756), ('assiswtance', 0.9222773313522339), ('assistanceto', 0.9199731349945068), ('wantassistance', 0.9171507954597473)]\n",
      "[('tomodify', 0.9890000224113464), ('modifythe', 0.9874863028526306), ('modifu', 0.983150839805603), ('modiby', 0.9607546925544739), ('edt', 0.9600982666015625), ('edit', 0.9564564228057861), ('modiy', 0.9467689990997314), ('editng', 0.9459406733512878), ('modigy', 0.940708577632904), ('correct', 0.9368051886558533)]\n",
      "[('tomodify', 0.9717926979064941), ('modify', 0.9684973359107971), ('modifythe', 0.9618499875068665), ('edt', 0.9343052506446838), ('edit', 0.9305016398429871), ('modifu', 0.9304682016372681), ('editng', 0.9227519035339355), ('modiby', 0.9155626893043518), ('correctdata', 0.909623384475708), ('correctly', 0.907287061214447)]\n"
     ]
    }
   ],
   "source": [
    "# FastText может работать с OOV (out-of-vocabulary) словами\n",
    "print(\"Вектор для существующего слова 'help':\")\n",
    "print(fasttext_model.wv['help'])\n",
    "\n",
    "# Вектор для несуществующего слова (FastText использует n-grams)\n",
    "fake_word = 'looooooooooooooool'\n",
    "print(f\"\\nВектор для OOV слова '{fake_word}':\")\n",
    "print(fasttext_model.wv[fake_word])\n",
    "\n",
    "# Похожие слова\n",
    "print(\"\\nПохожие слова на 'help':\")\n",
    "print(fasttext_model.wv.most_similar('help', topn=10))\n",
    "print(fasttext_model.wv.most_similar('modify'))\n",
    "print(fasttext_model.wv.most_similar('omdify'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b21dae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина вектора: (100,)\n"
     ]
    }
   ],
   "source": [
    "def document_vector_fasttext(model, doc):\n",
    "    # doc - это список слов ['i', 'need', 'omdify', 'address']\n",
    "    \n",
    "    # У FastText нет понятия \"незнакомое слово\", если оно состоит из букв,\n",
    "    # которые он видел, поэтому берем все слова\n",
    "    vectors = [model.wv[word] for word in doc]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "X_train_ft = np.array([document_vector_fasttext(fasttext_model, doc) for doc in X_train_tokenized_docs])\n",
    "X_val_ft = np.array([document_vector_fasttext(fasttext_model, doc) for doc in X_val_tokenized_docs])\n",
    "print(f\"Длина вектора: {X_train_ft[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34123976",
   "metadata": {},
   "source": [
    "### Классификация интентов\n",
    "**Модели**\n",
    "\n",
    "Точно пробуем из линейных моделей SVM и логистическую регрессию, но в целом можно поэкспериментировать.\n",
    "\n",
    "Для многоклассовой классификации - можно воспользоваться \"One-vs-Rest\" и \"One-vs-One\"\n",
    "\n",
    "**Метрики** \n",
    "\n",
    "f1-score, precision и recall (micro, macro и weighted - всё имплементировано в sklearn'е)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15780e09",
   "metadata": {},
   "source": [
    "Пара штрихов перед обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для multilabeled - делаем, чтобы всё хранилось в виде списков\n",
    "\n",
    "y_train['intent'] = y_train['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])\n",
    "y_val['intent'] = y_val['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])\n",
    "y_test['intent'] = y_test['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9bbf1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# бинарайзер - для мультилейбл классификатора\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train['intent'])\n",
    "y_val_bin = mlb.transform(y_val['intent'])\n",
    "y_test_bin = mlb.transform(y_test['intent'])\n",
    "\n",
    "y_train_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f60322df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [X_train_bag.toarray(), X_train_tfidf.toarray(), X_train_w2v, X_train_ft]\n",
    "X_val_list = [X_val_bag.toarray(), X_val_tfidf.toarray(), X_val_w2v, X_val_ft]\n",
    "\n",
    "X_train_val_list = [[X_train_bag.toarray(), X_val_bag.toarray()], \n",
    "                    [X_train_tfidf.toarray(), X_val_tfidf.toarray()], \n",
    "                    [X_train_w2v, X_val_w2v], # w2v здесь предобученная модель\n",
    "                    [X_train_ft, X_val_ft]\n",
    "                   ]\n",
    "\n",
    "# X_test_list = [X_test_bag.toarray(), X_test_tfidf.toarray(), X_test_w2v, X_test_ft]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc12b43",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3dd7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabeles классификации\n",
    "    # пока сделаем с дефолтными параметрами\n",
    "    model = MultiOutputClassifier(LogisticRegression(random_state=42))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    logreg_list.append(model)\n",
    "    y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fa5e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabel\n",
    "    # поменяли solver\n",
    "    model = MultiOutputClassifier(LogisticRegression(\n",
    "    random_state=42, \n",
    "    max_iter=1000,         # даем больше времени на обучение\n",
    "    class_weight='balanced', # чтобы не игнорировать редкие запросы\n",
    "    solver='liblinear'   # с saga не вышло\n",
    "    ))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    logreg_list.append(model)\n",
    "    y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dc0c298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "Micro F1: 0.9079699248120301\n",
      "Macro F1: 0.9109034654898237\n",
      "Weighted F1: 0.9113462918978267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.95       228\n",
      "           1       0.73      0.99      0.84       183\n",
      "           2       0.90      0.99      0.94       228\n",
      "           3       0.98      1.00      0.99       226\n",
      "           4       0.75      0.98      0.85       246\n",
      "           5       0.95      1.00      0.97       231\n",
      "           6       0.81      0.99      0.89       257\n",
      "           7       0.94      0.95      0.95       222\n",
      "           8       0.81      0.97      0.88       205\n",
      "           9       0.90      0.99      0.94       254\n",
      "          10       0.78      1.00      0.88       226\n",
      "          11       0.94      1.00      0.97       263\n",
      "          12       0.75      1.00      0.86       249\n",
      "          13       0.86      0.97      0.91       216\n",
      "          14       0.70      0.99      0.82       206\n",
      "          15       0.69      0.97      0.80       230\n",
      "          16       0.84      0.99      0.91       229\n",
      "          17       0.97      1.00      0.98       223\n",
      "          18       0.91      0.99      0.95       224\n",
      "          19       0.76      0.97      0.85       235\n",
      "          20       0.99      0.99      0.99       227\n",
      "          21       0.87      1.00      0.93       220\n",
      "          22       0.98      1.00      0.99       182\n",
      "          23       0.92      0.99      0.95       227\n",
      "          24       0.86      0.99      0.92       218\n",
      "          25       0.68      0.99      0.81       199\n",
      "          26       0.79      0.99      0.88       254\n",
      "\n",
      "   micro avg       0.84      0.99      0.91      6108\n",
      "   macro avg       0.85      0.99      0.91      6108\n",
      "weighted avg       0.85      0.99      0.91      6108\n",
      " samples avg       0.91      0.99      0.93      6108\n",
      "\n",
      "----------\n",
      "TF-IDF\n",
      "Micro F1: 0.8942535545023697\n",
      "Macro F1: 0.8974918431563895\n",
      "Weighted F1: 0.8982827967233903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       228\n",
      "           1       0.63      0.98      0.77       183\n",
      "           2       0.81      0.98      0.89       228\n",
      "           3       0.98      1.00      0.99       226\n",
      "           4       0.71      0.99      0.83       246\n",
      "           5       0.91      1.00      0.95       231\n",
      "           6       0.75      0.98      0.85       257\n",
      "           7       0.93      0.96      0.95       222\n",
      "           8       0.78      0.97      0.86       205\n",
      "           9       0.88      0.99      0.94       254\n",
      "          10       0.80      1.00      0.88       226\n",
      "          11       0.96      1.00      0.98       263\n",
      "          12       0.77      1.00      0.87       249\n",
      "          13       0.85      0.98      0.91       216\n",
      "          14       0.66      0.99      0.79       206\n",
      "          15       0.67      0.98      0.79       230\n",
      "          16       0.83      0.99      0.90       229\n",
      "          17       0.96      1.00      0.98       223\n",
      "          18       0.89      0.99      0.94       224\n",
      "          19       0.74      0.96      0.84       235\n",
      "          20       0.99      0.99      0.99       227\n",
      "          21       0.86      1.00      0.92       220\n",
      "          22       0.98      1.00      0.99       182\n",
      "          23       0.86      0.99      0.92       227\n",
      "          24       0.81      1.00      0.89       218\n",
      "          25       0.71      0.98      0.82       199\n",
      "          26       0.79      0.99      0.88       254\n",
      "\n",
      "   micro avg       0.82      0.99      0.89      6108\n",
      "   macro avg       0.83      0.99      0.90      6108\n",
      "weighted avg       0.83      0.99      0.90      6108\n",
      " samples avg       0.89      0.99      0.92      6108\n",
      "\n",
      "----------\n",
      "W2V\n",
      "Micro F1: 0.7585591329295506\n",
      "Macro F1: 0.7682338157838748\n",
      "Weighted F1: 0.7691022085031919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.96      0.80       228\n",
      "           1       0.41      0.98      0.58       183\n",
      "           2       0.58      0.96      0.72       228\n",
      "           3       0.87      0.98      0.93       226\n",
      "           4       0.51      0.98      0.67       246\n",
      "           5       0.66      0.98      0.79       231\n",
      "           6       0.53      0.96      0.68       257\n",
      "           7       0.69      0.96      0.80       222\n",
      "           8       0.62      0.96      0.76       205\n",
      "           9       0.85      0.99      0.92       254\n",
      "          10       0.57      0.98      0.72       226\n",
      "          11       0.66      0.97      0.78       263\n",
      "          12       0.62      0.98      0.76       249\n",
      "          13       0.69      0.98      0.81       216\n",
      "          14       0.46      0.94      0.62       206\n",
      "          15       0.51      0.97      0.67       230\n",
      "          16       0.60      0.96      0.74       229\n",
      "          17       0.91      1.00      0.95       223\n",
      "          18       0.72      0.99      0.83       224\n",
      "          19       0.51      0.95      0.66       235\n",
      "          20       0.93      1.00      0.96       227\n",
      "          21       0.63      0.99      0.77       220\n",
      "          22       0.77      1.00      0.87       182\n",
      "          23       0.67      0.98      0.80       227\n",
      "          24       0.62      0.97      0.75       218\n",
      "          25       0.54      0.95      0.68       199\n",
      "          26       0.56      0.98      0.71       254\n",
      "\n",
      "   micro avg       0.62      0.97      0.76      6108\n",
      "   macro avg       0.64      0.97      0.77      6108\n",
      "weighted avg       0.64      0.97      0.77      6108\n",
      " samples avg       0.76      0.98      0.82      6108\n",
      "\n",
      "----------\n",
      "FT\n",
      "Micro F1: 0.7921666886456548\n",
      "Macro F1: 0.8037827054464753\n",
      "Weighted F1: 0.8040129967224077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.98      0.84       228\n",
      "           1       0.44      0.99      0.61       183\n",
      "           2       0.67      0.96      0.79       228\n",
      "           3       0.98      1.00      0.99       226\n",
      "           4       0.55      0.97      0.70       246\n",
      "           5       0.84      1.00      0.91       231\n",
      "           6       0.61      0.98      0.75       257\n",
      "           7       0.72      0.97      0.83       222\n",
      "           8       0.72      0.96      0.83       205\n",
      "           9       0.83      0.98      0.90       254\n",
      "          10       0.62      0.99      0.77       226\n",
      "          11       0.58      0.98      0.73       263\n",
      "          12       0.63      0.99      0.77       249\n",
      "          13       0.71      0.98      0.82       216\n",
      "          14       0.51      0.99      0.67       206\n",
      "          15       0.55      0.98      0.71       230\n",
      "          16       0.59      0.98      0.74       229\n",
      "          17       0.96      1.00      0.98       223\n",
      "          18       0.73      0.99      0.84       224\n",
      "          19       0.51      0.98      0.67       235\n",
      "          20       0.95      1.00      0.97       227\n",
      "          21       0.75      0.99      0.85       220\n",
      "          22       0.92      1.00      0.96       182\n",
      "          23       0.79      0.98      0.87       227\n",
      "          24       0.62      0.98      0.76       218\n",
      "          25       0.49      0.97      0.66       199\n",
      "          26       0.65      0.99      0.79       254\n",
      "\n",
      "   micro avg       0.66      0.98      0.79      6108\n",
      "   macro avg       0.69      0.98      0.80      6108\n",
      "weighted avg       0.69      0.98      0.80      6108\n",
      " samples avg       0.79      0.99      0.85      6108\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "vecs = ['BoW', 'TF-IDF', 'W2V', 'FT']\n",
    "for i in range(4):\n",
    "    print(vecs[i])\n",
    "    print(\"Micro F1:\", f1_score(y_val_bin, y_pred_list[i], average='micro'))\n",
    "    print(\"Macro F1:\", f1_score(y_val_bin, y_pred_list[i], average='macro')) \n",
    "    print(\"Weighted F1:\", f1_score(y_val_bin, y_pred_list[i], average='weighted'))\n",
    "    print(classification_report(y_val_bin, y_pred_list[i]))\n",
    "    print('-'*10)\n",
    "\n",
    "    \n",
    "# модель предсказала все нули???\n",
    "# стоит ли сделать , zero_division=0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08f0a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация тестовой выборки...\n",
      "Размер X_test: (6055, 3277)\n",
      "Размер y_test: (6055, 27)\n",
      "Предсказание на тестовых данных...\n",
      "----------------------------------------\n",
      "ИТОГОВЫЕ МЕТРИКИ НА TEST (Bag of Words)\n",
      "----------------------------------------\n",
      "Micro F1:    0.9082\n",
      "Macro F1:    0.9111\n",
      "Weighted F1: 0.9113\n",
      "\n",
      "Полный отчет:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       246\n",
      "           1       0.76      0.98      0.86       228\n",
      "           2       0.86      0.98      0.92       210\n",
      "           3       0.98      0.99      0.99       258\n",
      "           4       0.76      0.99      0.86       217\n",
      "           5       0.94      1.00      0.97       248\n",
      "           6       0.80      0.98      0.88       262\n",
      "           7       0.94      0.94      0.94       193\n",
      "           8       0.85      0.97      0.91       240\n",
      "           9       0.90      0.98      0.94       249\n",
      "          10       0.81      0.98      0.89       202\n",
      "          11       0.91      1.00      0.95       222\n",
      "          12       0.77      0.99      0.86       238\n",
      "          13       0.84      1.00      0.91       215\n",
      "          14       0.72      0.96      0.82       226\n",
      "          15       0.70      0.98      0.82       204\n",
      "          16       0.81      1.00      0.90       212\n",
      "          17       0.94      0.99      0.97       238\n",
      "          18       0.92      0.99      0.96       204\n",
      "          19       0.71      0.99      0.83       234\n",
      "          20       0.99      0.99      0.99       240\n",
      "          21       0.85      0.98      0.91       201\n",
      "          22       0.99      1.00      0.99       189\n",
      "          23       0.92      1.00      0.96       229\n",
      "          24       0.87      0.99      0.92       211\n",
      "          25       0.70      0.99      0.82       237\n",
      "          26       0.82      1.00      0.90       278\n",
      "\n",
      "   micro avg       0.84      0.99      0.91      6131\n",
      "   macro avg       0.85      0.99      0.91      6131\n",
      "weighted avg       0.85      0.99      0.91      6131\n",
      " samples avg       0.91      0.99      0.93      6131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Берем векторизатор Bag of Words, который мы обучили на Train.\n",
    "# используем только .transform()\n",
    "print(\"Векторизация тестовой выборки...\")\n",
    "X_test_bag = vec_bag.transform(X_test['clean_instruction']).toarray()\n",
    "\n",
    "# y_test_bin = mlb.transform(y_test['intent'])\n",
    "print(f\"Размер X_test: {X_test_bag.shape}\")\n",
    "print(f\"Размер y_test: {y_test_bin.shape}\")\n",
    "\n",
    "\n",
    "# так как в цикле векторов ['BoW', 'TF-IDF'...] BoW шел первым:\n",
    "model_bow = logreg_list[0]\n",
    "\n",
    "print(\"Предсказание на тестовых данных...\")\n",
    "y_pred_test = model_bow.predict(X_test_bag)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"ИТОГОВЫЕ МЕТРИКИ НА TEST (Bag of Words)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"Micro F1:    {f1_score(y_test_bin, y_pred_test, average='micro'):.4f}\")\n",
    "print(f\"Macro F1:    {f1_score(y_test_bin, y_pred_test, average='macro'):.4f}\")\n",
    "print(f\"Weighted F1: {f1_score(y_test_bin, y_pred_test, average='weighted'):.4f}\")\n",
    "print(\"\\nПолный отчет:\")\n",
    "print(classification_report(y_test_bin, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625d77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ДОПОЛНИТЕЛЬНЫЕ МЕТРИКИ (BoW) ---\n",
      "Exact Match Ratio (Полная автоматизация): 0.8296\n",
      "Jaccard Score (Степень похожести):        0.9057\n",
      "At Least One Match (Полезное действие):   0.9871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, jaccard_score\n",
    " \n",
    "# 1. Exact Match Ratio (Идеальное совпадение)\n",
    "# Показывает % тикетов, где модель не ошиблась ни в одной запятой\n",
    "exact_match = accuracy_score(y_test_bin, y_pred_test)\n",
    "\n",
    "# 2. Jaccard Score (samples average)\n",
    "# Показывает средний % перекрытия правильных и предсказанных тегов\n",
    "jaccard = jaccard_score(y_test_bin, y_pred_test, average='samples')\n",
    "\n",
    "# 3. Кастомная метрика: \"Хотя бы один верный\" (At Least One Match)\n",
    "def at_least_one_accuracy(y_true, y_pred):\n",
    "    # Умножаем матрицы: если есть совпадение 1*1, будет > 0\n",
    "    matches = (y_true * y_pred).sum(axis=1)\n",
    "    # Считаем долю строк, где сумма > 0\n",
    "    return (matches > 0).mean()\n",
    "\n",
    "at_least_one = at_least_one_accuracy(y_test_bin, y_pred_test)\n",
    "\n",
    "print(f\"--- ДОПОЛНИТЕЛЬНЫЕ МЕТРИКИ (BoW) ---\")\n",
    "print(f\"Exact Match Ratio (Полная автоматизация): {exact_match:.4f}\")\n",
    "print(f\"Jaccard Score (Степень похожести):        {jaccard:.4f}\")\n",
    "print(f\"At Least One Match (Полезное действие):   {at_least_one:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ff8bf",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "def6261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "\n",
    "svc_list = []\n",
    "y_svc_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabel классификации\n",
    "    # поменяли solver\n",
    "    model = MultiOutputClassifier(LinearSVC(\n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        max_iter=2000\n",
    "    ))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    svc_list.append(model)\n",
    "    y_svc_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f12e13f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "Micro F1: 0.9317438055165965\n",
      "Macro F1: 0.9336784976578885\n",
      "Weighted F1: 0.9336853071175403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       228\n",
      "           1       0.88      0.95      0.91       183\n",
      "           2       0.93      0.98      0.95       228\n",
      "           3       0.99      0.99      0.99       226\n",
      "           4       0.77      0.97      0.86       246\n",
      "           5       0.97      0.99      0.98       231\n",
      "           6       0.89      0.97      0.93       257\n",
      "           7       0.96      0.96      0.96       222\n",
      "           8       0.81      0.95      0.87       205\n",
      "           9       0.91      0.96      0.94       254\n",
      "          10       0.85      0.98      0.91       226\n",
      "          11       0.98      0.99      0.99       263\n",
      "          12       0.81      1.00      0.89       249\n",
      "          13       0.90      0.96      0.93       216\n",
      "          14       0.85      0.98      0.91       206\n",
      "          15       0.74      0.96      0.83       230\n",
      "          16       0.89      0.99      0.94       229\n",
      "          17       0.98      1.00      0.99       223\n",
      "          18       0.94      0.99      0.96       224\n",
      "          19       0.82      0.96      0.89       235\n",
      "          20       0.99      1.00      0.99       227\n",
      "          21       0.87      0.99      0.93       220\n",
      "          22       0.99      1.00      1.00       182\n",
      "          23       0.95      0.98      0.97       227\n",
      "          24       0.96      0.98      0.97       218\n",
      "          25       0.75      0.98      0.85       199\n",
      "          26       0.85      0.98      0.91       254\n",
      "\n",
      "   micro avg       0.89      0.98      0.93      6108\n",
      "   macro avg       0.90      0.98      0.93      6108\n",
      "weighted avg       0.90      0.98      0.93      6108\n",
      " samples avg       0.93      0.98      0.95      6108\n",
      "\n",
      "----------\n",
      "TF-IDF\n",
      "Micro F1: 0.9240613676663326\n",
      "Macro F1: 0.926130742073881\n",
      "Weighted F1: 0.926227914057147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       228\n",
      "           1       0.82      0.96      0.88       183\n",
      "           2       0.91      0.98      0.94       228\n",
      "           3       0.99      1.00      0.99       226\n",
      "           4       0.77      0.97      0.86       246\n",
      "           5       0.95      0.99      0.97       231\n",
      "           6       0.83      0.96      0.89       257\n",
      "           7       0.96      0.96      0.96       222\n",
      "           8       0.81      0.96      0.88       205\n",
      "           9       0.90      0.98      0.93       254\n",
      "          10       0.88      0.99      0.93       226\n",
      "          11       0.97      1.00      0.98       263\n",
      "          12       0.81      1.00      0.89       249\n",
      "          13       0.89      0.96      0.92       216\n",
      "          14       0.77      0.98      0.86       206\n",
      "          15       0.71      0.96      0.82       230\n",
      "          16       0.88      0.97      0.92       229\n",
      "          17       0.97      1.00      0.99       223\n",
      "          18       0.93      0.99      0.96       224\n",
      "          19       0.82      0.96      0.88       235\n",
      "          20       0.98      1.00      0.99       227\n",
      "          21       0.88      1.00      0.93       220\n",
      "          22       0.99      1.00      0.99       182\n",
      "          23       0.94      0.99      0.96       227\n",
      "          24       0.95      1.00      0.98       218\n",
      "          25       0.74      0.97      0.84       199\n",
      "          26       0.84      0.98      0.90       254\n",
      "\n",
      "   micro avg       0.87      0.98      0.92      6108\n",
      "   macro avg       0.88      0.98      0.93      6108\n",
      "weighted avg       0.88      0.98      0.93      6108\n",
      " samples avg       0.92      0.98      0.94      6108\n",
      "\n",
      "----------\n",
      "W2V\n",
      "Micro F1: 0.7632781819352347\n",
      "Macro F1: 0.7730803105789396\n",
      "Weighted F1: 0.7740018557452377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.96      0.81       228\n",
      "           1       0.41      0.98      0.58       183\n",
      "           2       0.58      0.96      0.72       228\n",
      "           3       0.85      1.00      0.92       226\n",
      "           4       0.52      0.98      0.68       246\n",
      "           5       0.68      0.98      0.80       231\n",
      "           6       0.54      0.96      0.69       257\n",
      "           7       0.68      0.96      0.80       222\n",
      "           8       0.64      0.96      0.77       205\n",
      "           9       0.85      0.98      0.91       254\n",
      "          10       0.61      0.99      0.75       226\n",
      "          11       0.67      0.96      0.79       263\n",
      "          12       0.63      0.99      0.77       249\n",
      "          13       0.68      0.98      0.80       216\n",
      "          14       0.44      0.93      0.60       206\n",
      "          15       0.51      0.96      0.67       230\n",
      "          16       0.58      0.95      0.72       229\n",
      "          17       0.92      0.99      0.95       223\n",
      "          18       0.74      0.98      0.84       224\n",
      "          19       0.52      0.95      0.67       235\n",
      "          20       0.94      1.00      0.97       227\n",
      "          21       0.64      0.99      0.78       220\n",
      "          22       0.81      0.99      0.89       182\n",
      "          23       0.70      0.97      0.81       227\n",
      "          24       0.63      0.95      0.76       218\n",
      "          25       0.54      0.95      0.69       199\n",
      "          26       0.58      0.98      0.73       254\n",
      "\n",
      "   micro avg       0.63      0.97      0.76      6108\n",
      "   macro avg       0.65      0.97      0.77      6108\n",
      "weighted avg       0.65      0.97      0.77      6108\n",
      " samples avg       0.77      0.97      0.83      6108\n",
      "\n",
      "----------\n",
      "FT\n",
      "Micro F1: 0.8211259907078436\n",
      "Macro F1: 0.8300999077790424\n",
      "Weighted F1: 0.830814649938103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86       228\n",
      "           1       0.47      0.99      0.64       183\n",
      "           2       0.72      0.97      0.83       228\n",
      "           3       0.98      1.00      0.99       226\n",
      "           4       0.58      0.98      0.73       246\n",
      "           5       0.88      1.00      0.93       231\n",
      "           6       0.66      0.97      0.78       257\n",
      "           7       0.76      0.97      0.85       222\n",
      "           8       0.75      0.96      0.84       205\n",
      "           9       0.84      0.98      0.91       254\n",
      "          10       0.67      0.99      0.80       226\n",
      "          11       0.69      0.99      0.82       263\n",
      "          12       0.68      0.98      0.80       249\n",
      "          13       0.72      0.96      0.82       216\n",
      "          14       0.52      0.99      0.68       206\n",
      "          15       0.59      0.98      0.74       230\n",
      "          16       0.64      0.98      0.78       229\n",
      "          17       0.96      1.00      0.98       223\n",
      "          18       0.79      0.99      0.88       224\n",
      "          19       0.58      0.99      0.73       235\n",
      "          20       0.97      1.00      0.98       227\n",
      "          21       0.77      0.99      0.87       220\n",
      "          22       0.94      1.00      0.97       182\n",
      "          23       0.81      0.99      0.89       227\n",
      "          24       0.72      0.98      0.83       218\n",
      "          25       0.53      0.96      0.68       199\n",
      "          26       0.69      0.98      0.81       254\n",
      "\n",
      "   micro avg       0.70      0.98      0.82      6108\n",
      "   macro avg       0.73      0.98      0.83      6108\n",
      "weighted avg       0.73      0.98      0.83      6108\n",
      " samples avg       0.83      0.99      0.87      6108\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "vecs = ['BoW', 'TF-IDF', 'W2V', 'FT']\n",
    "for i in range(4):\n",
    "    print(vecs[i])\n",
    "    print(\"Micro F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='micro'))\n",
    "    print(\"Macro F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='macro')) \n",
    "    print(\"Weighted F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='weighted'))\n",
    "    print(classification_report(y_val_bin, y_svc_pred_list[i]))\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1a0a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация тестовой выборки...\n",
      "Размер X_test: (6055, 3277)\n",
      "Размер y_test: (6055, 27)\n",
      "Предсказание на тестовых данных...\n",
      "----------------------------------------\n",
      "ИТОГОВЫЕ МЕТРИКИ НА TEST (Bag of Words)\n",
      "----------------------------------------\n",
      "Micro F1:    0.9349\n",
      "Macro F1:    0.9365\n",
      "Weighted F1: 0.9368\n",
      "\n",
      "Полный отчет:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       246\n",
      "           1       0.90      0.96      0.93       228\n",
      "           2       0.94      0.94      0.94       210\n",
      "           3       1.00      1.00      1.00       258\n",
      "           4       0.80      0.96      0.87       217\n",
      "           5       0.98      1.00      0.99       248\n",
      "           6       0.86      0.95      0.90       262\n",
      "           7       0.97      0.94      0.96       193\n",
      "           8       0.89      0.96      0.92       240\n",
      "           9       0.92      0.98      0.95       249\n",
      "          10       0.85      0.98      0.91       202\n",
      "          11       0.96      0.98      0.97       222\n",
      "          12       0.82      0.98      0.90       238\n",
      "          13       0.87      0.98      0.92       215\n",
      "          14       0.85      0.96      0.90       226\n",
      "          15       0.73      0.98      0.84       204\n",
      "          16       0.88      1.00      0.93       212\n",
      "          17       0.96      0.99      0.98       238\n",
      "          18       0.98      0.98      0.98       204\n",
      "          19       0.79      0.97      0.87       234\n",
      "          20       1.00      0.99      0.99       240\n",
      "          21       0.87      0.97      0.92       201\n",
      "          22       0.99      0.99      0.99       189\n",
      "          23       0.98      1.00      0.99       229\n",
      "          24       0.96      0.97      0.96       211\n",
      "          25       0.77      0.98      0.86       237\n",
      "          26       0.88      1.00      0.93       278\n",
      "\n",
      "   micro avg       0.90      0.98      0.93      6131\n",
      "   macro avg       0.90      0.98      0.94      6131\n",
      "weighted avg       0.90      0.98      0.94      6131\n",
      " samples avg       0.93      0.98      0.95      6131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Берем векторизатор Bag of Words, который мы обучили на Train.\n",
    "# используем только .transform()\n",
    "print(\"Векторизация тестовой выборки...\")\n",
    "X_test_bag = vec_bag.transform(X_test['clean_instruction']).toarray()\n",
    "\n",
    "# y_test_bin = mlb.transform(y_test['intent'])\n",
    "print(f\"Размер X_test: {X_test_bag.shape}\")\n",
    "print(f\"Размер y_test: {y_test_bin.shape}\")\n",
    "\n",
    "\n",
    "# так как в цикле векторов ['BoW', 'TF-IDF'...] BoW шел первым:\n",
    "model_bow = svc_list[0]\n",
    "\n",
    "print(\"Предсказание на тестовых данных...\")\n",
    "y_pred_test = model_bow.predict(X_test_bag)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"ИТОГОВЫЕ МЕТРИКИ НА TEST (Bag of Words)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"Micro F1:    {f1_score(y_test_bin, y_pred_test, average='micro'):.4f}\")\n",
    "print(f\"Macro F1:    {f1_score(y_test_bin, y_pred_test, average='macro'):.4f}\")\n",
    "print(f\"Weighted F1: {f1_score(y_test_bin, y_pred_test, average='weighted'):.4f}\")\n",
    "print(\"\\nПолный отчет:\")\n",
    "print(classification_report(y_test_bin, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "909bd7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ДОПОЛНИТЕЛЬНЫЕ МЕТРИКИ (BoW) ---\n",
      "Exact Match Ratio (Полная автоматизация): 0.8880\n",
      "Jaccard Score (Степень похожести):        0.9319\n",
      "At Least One Match (Полезное действие):   0.9782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, jaccard_score\n",
    "\n",
    "# 1. Exact Match Ratio (Идеальное совпадение)\n",
    "# Показывает % тикетов, где модель не ошиблась ни в одной запятой\n",
    "exact_match = accuracy_score(y_test_bin, y_pred_test)\n",
    "\n",
    "# 2. Jaccard Score (samples average)\n",
    "# Показывает средний % перекрытия правильных и предсказанных тегов\n",
    "jaccard = jaccard_score(y_test_bin, y_pred_test, average='samples')\n",
    "\n",
    "# 3. Кастомная метрика: \"Хотя бы один верный\" (At Least One Match)\n",
    "def at_least_one_accuracy(y_true, y_pred):\n",
    "    # Умножаем матрицы: если есть совпадение 1*1, будет > 0\n",
    "    matches = (y_true * y_pred).sum(axis=1)\n",
    "    # Считаем долю строк, где сумма > 0\n",
    "    return (matches > 0).mean()\n",
    "\n",
    "at_least_one = at_least_one_accuracy(y_test_bin, y_pred_test)\n",
    "\n",
    "print(f\"--- ДОПОЛНИТЕЛЬНЫЕ МЕТРИКИ (BoW) ---\")\n",
    "print(f\"Exact Match Ratio (Полная автоматизация): {exact_match:.4f}\")\n",
    "print(f\"Jaccard Score (Степень похожести):        {jaccard:.4f}\")\n",
    "print(f\"At Least One Match (Полезное действие):   {at_least_one:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e1390e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_search = X_train_bag\n",
    "y_train_search = y_train_bin\n",
    "\n",
    "base_model = MultiOutputClassifier(LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'))\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10],\n",
    "    'estimator__class_weight': [None, 'balanced'],\n",
    "    'estimator__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_search, y_train_search)\n",
    "\n",
    "print(\"\\nЛучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший F1-score на кросс-валидации:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val_bag)\n",
    "\n",
    "print(\"\\nОтчет лучшей модели:\")\n",
    "print(classification_report(y_val_bin, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8846ac",
   "metadata": {},
   "source": [
    "## Вопросы на подумать\n",
    "- **подозрительно хороший скор** - кажется, я где-то жестоко неправ\n",
    "- мультикласс - в нашем случае лучше onehotencode'ить каждую вариацию? или рассматривать комбинацию интентов как отдельный класс? unique-значения и играться порогами для моделей, чтобы было 1-2 метки предсказанных\n",
    "- как бы чистить выбросы типо \"question abobut, aaaaabout\" и прочее? или их не надо чистить, считаем это нормальным шумом, который вполне реален? не теряет ли модель качество в этом шуме?\n",
    "- ~~надо преобразование для трейна и валидации делать одной или разными векторайзерами? по логике кажется, что одним~~\n",
    "- **пороги вероятностей для моделей**\n",
    "\n",
    "### Мысли\n",
    "- не сходится логрег и оч долго обучается - что-то странное\n",
    "- точно нужно попробовать подбор гиперпараметров, с помощью GridSearchCV к примеру\n",
    "- модель предсказала все нули??? стоит ли сделать , zero_division=0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980d774",
   "metadata": {},
   "source": [
    "- думаем про стоп-слова и комменты повыше\n",
    "- можно подумать над доп. метриками (взвешенный accuracy, кастомные)\n",
    "- перебераем параметры, смотрим на метрики, мб графики\n",
    "- попробовать ещё моделей\n",
    "- пайплайн обучения, пайплайн применения - всё объединить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cee26b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 27)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 0][0], \n",
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "26077919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 27)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "0e81ccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 27)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 0][0], \n",
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4bd253f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 27)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "83830184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) >= 2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8f6987ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409498d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
