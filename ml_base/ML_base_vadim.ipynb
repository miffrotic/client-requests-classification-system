{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb801382",
   "metadata": {},
   "source": [
    "## Ноут-шаблон под обучение моделей\n",
    "> **Важно: обязательно фиксируем рандом (`seed=42`) для всех библиотек и методов, где под капотом подразумевается рандом!** \n",
    "\n",
    "> **Почему это важно:** для гарантии консистентности данных при сравнении разных моделей и решений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "edbec7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# фиксирую то, что импортнул\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0c14c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем необходимые ресурсы (выполнить один раз)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c4d45",
   "metadata": {},
   "source": [
    "### Импорт датасета и сплит на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "974acd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flags</th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "      <th>intent</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>question about cancelling order {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've understood you have a question regarding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BQZ</td>\n",
       "      <td>i have a question about cancelling oorder {{Or...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I've been informed that you have a question ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BLQZ</td>\n",
       "      <td>i need help cancelling puchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I can sense that you're seeking assistance wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BL</td>\n",
       "      <td>I need to cancel purchase {{Order Number}}</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I understood that you need assistance with can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCELN</td>\n",
       "      <td>I cannot afford this order, cancel purchase {{...</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>I'm sensitive to the fact that you're facing f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flags                                        instruction category  \\\n",
       "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
       "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
       "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
       "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
       "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
       "\n",
       "         intent                                           response  \n",
       "0  cancel_order  I've understood you have a question regarding ...  \n",
       "1  cancel_order  I've been informed that you have a question ab...  \n",
       "2  cancel_order  I can sense that you're seeking assistance wit...  \n",
       "3  cancel_order  I understood that you need assistance with can...  \n",
       "4  cancel_order  I'm sensitive to the fact that you're facing f...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем датасет\n",
    "df = pd.read_csv(\"customer_support_dataset_generated.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba340d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flags                                                        BLC\n",
       "instruction    I want to create an account and also see my de...\n",
       "category                                       ACCOUNT, DELIVERY\n",
       "intent                          create_account, delivery_options\n",
       "response       Certainly! To create a new account, please hea...\n",
       "Name: 28120, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[28120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffdc56",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fbbe5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обязательный блок: исправляем дублящиеся категории и интенты\n",
    "def process_category_unique(value):\n",
    "    if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "        try:\n",
    "            categories = ast.literal_eval(value)\n",
    "            unique_categories = list(set(categories))\n",
    "            # Если осталась одна категория - возвращаем как строку\n",
    "            if len(unique_categories) == 1:\n",
    "                return unique_categories[0]\n",
    "            else:\n",
    "                return ', '.join(sorted(unique_categories))\n",
    "        except:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "    \n",
    "    \n",
    "df['category'] = df['category'].apply(process_category_unique).astype(str)\n",
    "df['intent'] = df['intent'].apply(process_category_unique).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "55a9bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flags                                                              BLC\n",
       "instruction          I want to create an account and also see my de...\n",
       "category                                             ACCOUNT, DELIVERY\n",
       "intent                                create_account, delivery_options\n",
       "response             Certainly! To create a new account, please hea...\n",
       "clean_instruction         want create account also see delivery option\n",
       "Name: 28120, dtype: object"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[28120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c43bc4",
   "metadata": {},
   "source": [
    "**Дальше я делаю подготовку текстовок обращений**\n",
    "\n",
    "В целом, этот блок на ваше усмотрение можно кастомить, если я что-то забыл учесть, или у вас появилась классная идея, ~~или я где-то жестоко не прав и ошибся~~, но базовый минимум здесь - дропнуть английские стоп-слова и пунктуацию, привести слова к леммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9eef465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так было в моём EDA\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))  # или 'russian', 'spanish' и т.д.\n",
    "\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     # Удаляем различные шаблоны с фигурными скобками\n",
    "#     patterns_to_remove = [r'{{.*?}}']\n",
    "    \n",
    "#     for pattern in patterns_to_remove:\n",
    "#         text = re.sub(pattern, '', text)\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# df['clean_instruction'] = df['instruction'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0cd99a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, language='english', use_lemmatization=True, custom_stopwords=None):\n",
    "        \n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords)\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer() if use_lemmatization else None\n",
    "        self.stemmer = PorterStemmer() if not use_lemmatization else None\n",
    "        \n",
    "        # Расширенные шаблоны для удаления\n",
    "        self.patterns_to_remove = [\n",
    "            r'{{.*?}}',  # шаблоны с фигурными скобками\n",
    "            r'\\[.*?\\]',  # квадратные скобки\n",
    "            r'<.*?>',    # HTML теги\n",
    "            r'http\\S+',  # URL\n",
    "            r'@\\w+',     # упоминания\n",
    "            r'#\\w+',     # хэштеги\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Приведение к нижнему регистру\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Удаление шаблонов\n",
    "        for pattern in self.patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        \n",
    "        # Удаление пунктуации и цифр\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "        \n",
    "        # Токенизация\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Фильтрация и нормализация\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if (len(token) > 1 and                    # убираем одиночные символы\n",
    "                token.isalpha() and                   # только буквы\n",
    "                token not in self.stop_words):        # не стоп-слово\n",
    "                \n",
    "                if self.lemmatizer:\n",
    "                    token = self.lemmatizer.lemmatize(token)\n",
    "                elif self.stemmer:\n",
    "                    token = self.stemmer.stem(token)\n",
    "                \n",
    "                filtered_tokens.append(token)\n",
    "        \n",
    "        return \" \".join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1bdcb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "# Создаем препроцессор\n",
    "preprocessor = TextPreprocessor(language='english', use_lemmatization=True)\n",
    "\n",
    "df['clean_instruction'] = df['instruction'].apply(preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d66865be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.stop_words # для понимания, что там лежит \n",
    "\n",
    "# no, not -> вероятно не нужно исключать, подумать\n",
    "# i do **not** want to cancel order, i want to change shipping address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8db179d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>clean_instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>I do not know how to check the bills from {{Pe...</td>\n",
       "      <td>know check bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30359</th>\n",
       "      <td>I need to begin the process of creating an acc...</td>\n",
       "      <td>need begin process creating account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17043</th>\n",
       "      <td>I need assistance to sign up to your newsletter</td>\n",
       "      <td>need assistance sign newsletter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>can you help me to restore the pass of my acco...</td>\n",
       "      <td>help restore pas account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>i need help canceling order {{Order Number}}</td>\n",
       "      <td>need help canceling order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7119</th>\n",
       "      <td>how to lodge a reclamation against your company?</td>\n",
       "      <td>lodge reclamation company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21008</th>\n",
       "      <td>i cant sign up i need to inform of signup erroprs</td>\n",
       "      <td>cant sign need inform signup erroprs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40362</th>\n",
       "      <td>I would like to place an order for some flowers.</td>\n",
       "      <td>would like place order flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24452</th>\n",
       "      <td>i want assistance using tghe fucking {{Account...</td>\n",
       "      <td>want assistance using tghe fucking account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24274</th>\n",
       "      <td>can you help me using the premium profile?</td>\n",
       "      <td>help using premium profile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27164</th>\n",
       "      <td>Hey, I need to get signed up.</td>\n",
       "      <td>hey need get signed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>what are the penalties for breaking the contract?</td>\n",
       "      <td>penalty breaking contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7180</th>\n",
       "      <td>assistance making a consumer reclamation again...</td>\n",
       "      <td>assistance making consumer reclamation ur company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34957</th>\n",
       "      <td>I want to retract my order.</td>\n",
       "      <td>want retract order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>I have to check the termination fee, how can I...</td>\n",
       "      <td>check termination fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26028</th>\n",
       "      <td>I expect an rebate of {{Refund Amount}} dollars</td>\n",
       "      <td>expect rebate dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>I need to make a complaint against your business</td>\n",
       "      <td>need make complaint business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32332</th>\n",
       "      <td>Cancellation fee inquiry.</td>\n",
       "      <td>cancellation fee inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27906</th>\n",
       "      <td>Delivery period status</td>\n",
       "      <td>delivery period status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>uhave a free number to talk with customer assi...</td>\n",
       "      <td>uhave free number talk customer assistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>how could I swap several items of purchase {{O...</td>\n",
       "      <td>could swap several item purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22212</th>\n",
       "      <td>can you help me leave a review about your comp...</td>\n",
       "      <td>help leave review company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36006</th>\n",
       "      <td>Add me to your email subscription list.</td>\n",
       "      <td>add email subscription list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>help me deleting an item from purchase {{Order...</td>\n",
       "      <td>help deleting item purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23776</th>\n",
       "      <td>I have problems setting a secondary shipping a...</td>\n",
       "      <td>problem setting secondary shipping address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "4513   I do not know how to check the bills from {{Pe...   \n",
       "30359  I need to begin the process of creating an acc...   \n",
       "17043    I need assistance to sign up to your newsletter   \n",
       "20542  can you help me to restore the pass of my acco...   \n",
       "715         i need help canceling order {{Order Number}}   \n",
       "7119    how to lodge a reclamation against your company?   \n",
       "21008  i cant sign up i need to inform of signup erroprs   \n",
       "40362   I would like to place an order for some flowers.   \n",
       "24452  i want assistance using tghe fucking {{Account...   \n",
       "24274         can you help me using the premium profile?   \n",
       "27164                      Hey, I need to get signed up.   \n",
       "3851   what are the penalties for breaking the contract?   \n",
       "7180   assistance making a consumer reclamation again...   \n",
       "34957                        I want to retract my order.   \n",
       "3317   I have to check the termination fee, how can I...   \n",
       "26028    I expect an rebate of {{Refund Amount}} dollars   \n",
       "7904    I need to make a complaint against your business   \n",
       "32332                          Cancellation fee inquiry.   \n",
       "27906                             Delivery period status   \n",
       "8875   uhave a free number to talk with customer assi...   \n",
       "1630   how could I swap several items of purchase {{O...   \n",
       "22212  can you help me leave a review about your comp...   \n",
       "36006            Add me to your email subscription list.   \n",
       "1688   help me deleting an item from purchase {{Order...   \n",
       "23776  I have problems setting a secondary shipping a...   \n",
       "\n",
       "                                       clean_instruction  \n",
       "4513                                     know check bill  \n",
       "30359                need begin process creating account  \n",
       "17043                    need assistance sign newsletter  \n",
       "20542                           help restore pas account  \n",
       "715                            need help canceling order  \n",
       "7119                           lodge reclamation company  \n",
       "21008               cant sign need inform signup erroprs  \n",
       "40362                      would like place order flower  \n",
       "24452         want assistance using tghe fucking account  \n",
       "24274                         help using premium profile  \n",
       "27164                                hey need get signed  \n",
       "3851                           penalty breaking contract  \n",
       "7180   assistance making consumer reclamation ur company  \n",
       "34957                                 want retract order  \n",
       "3317                               check termination fee  \n",
       "26028                               expect rebate dollar  \n",
       "7904                        need make complaint business  \n",
       "32332                           cancellation fee inquiry  \n",
       "27906                             delivery period status  \n",
       "8875          uhave free number talk customer assistance  \n",
       "1630                    could swap several item purchase  \n",
       "22212                          help leave review company  \n",
       "36006                        add email subscription list  \n",
       "1688                         help deleting item purchase  \n",
       "23776         problem setting secondary shipping address  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['instruction', 'clean_instruction']].sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536f850",
   "metadata": {},
   "source": [
    "### Сплит выборки\n",
    "Разбиваем на:\n",
    "- `train` - на нём обучаем модели\n",
    "- `validation` - на нём \"тюним\" модели, настраимаем гиперпараметры (когда GridSearch делаем, к примеру)\n",
    "- `test` - финальная выборка для оценки качества  Тестовый набор используется **ТОЛЬКО ОДИН РАЗ** в самом конце! При обучении и подборе параметров модель ничего не должна знать из этого датасета.\n",
    "\n",
    "Предлагаю бить в пропорции **70/15/15**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe8749",
   "metadata": {},
   "source": [
    "- train\n",
    "- validation для экспериментов\n",
    "- test откладываем и забываем - его доразбить для имитации сервиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39cf37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, \n",
    "                         y, \n",
    "                         val_size=0.15, \n",
    "                         test_size=0.15, \n",
    "                         random_state=42 # !!!\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Разделяет данные на train, validation и test наборы\n",
    "    \"\"\"\n",
    "\n",
    "    # Сначала отделяем test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "        # stratify=y  # есть классы всего лишь с 1 примером, поэтому просто шафлим\n",
    "    )\n",
    "    \n",
    "    # Затем разделяем на train и validation\n",
    "    relative_val_size = val_size / (1 - test_size)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, \n",
    "        y_train_val, \n",
    "        test_size=relative_val_size, \n",
    "        random_state=random_state, \n",
    "        shuffle=True\n",
    "        # stratify=y_train_val  # есть классы всего лишь с 1 примером, поэтому просто шафлим\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Получаем выборки\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=df.drop(['intent'], axis=1), \n",
    "    y=df[['intent']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "678c69f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40366, 6), (28256, 5), (6055, 5), (6055, 5))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6ae35",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "Пробуем следующие методы:\n",
    "- Bag of Words (мешок слов)\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "- FastText\n",
    "\n",
    "Статья, которой можно вдохновиться: https://habr.com/ru/articles/778048/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a002b8",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a20d2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aa  aaa  aabout  aaccount  aailable  aasistance  aat  abailable  abc  \\\n",
      "0       0    0       0         0         0           0    0          0    0   \n",
      "1       0    0       0         0         0           0    0          0    0   \n",
      "2       0    0       0         0         0           0    0          0    0   \n",
      "3       0    0       0         0         0           0    0          0    0   \n",
      "4       0    0       0         0         0           0    0          0    0   \n",
      "...    ..  ...     ...       ...       ...         ...  ...        ...  ...   \n",
      "28251   0    0       0         0         0           0    0          0    0   \n",
      "28252   0    0       0         0         0           0    0          0    0   \n",
      "28253   0    0       0         0         0           0    0          0    0   \n",
      "28254   0    0       0         0         0           0    0          0    0   \n",
      "28255   0    0       0         0         0           0    0          0    0   \n",
      "\n",
      "       abcxyz  ...  yor  yoru  youe  youhave  youhelp  youir  youshow  ypur  \\\n",
      "0           0  ...    0     0     0        0        0      0        0     0   \n",
      "1           0  ...    0     0     0        0        0      0        0     0   \n",
      "2           0  ...    0     0     0        0        0      0        0     0   \n",
      "3           0  ...    0     0     0        0        0      0        0     0   \n",
      "4           0  ...    0     0     0        0        0      0        0     0   \n",
      "...       ...  ...  ...   ...   ...      ...      ...    ...      ...   ...   \n",
      "28251       0  ...    0     0     0        0        0      0        0     0   \n",
      "28252       0  ...    0     0     0        0        0      0        0     0   \n",
      "28253       0  ...    0     0     0        0        0      0        0     0   \n",
      "28254       0  ...    0     0     0        0        0      0        0     0   \n",
      "28255       0  ...    0     0     0        0        0      0        0     0   \n",
      "\n",
      "       yur  zelle  \n",
      "0        0      0  \n",
      "1        0      0  \n",
      "2        0      0  \n",
      "3        0      0  \n",
      "4        0      0  \n",
      "...    ...    ...  \n",
      "28251    0      0  \n",
      "28252    0      0  \n",
      "28253    0      0  \n",
      "28254    0      0  \n",
      "28255    0      0  \n",
      "\n",
      "[28256 rows x 3420 columns]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "\n",
    "vec_bag = CountVectorizer()\n",
    "X_train_bag = vec_bag.fit_transform(X_train['clean_instruction'])\n",
    "X_val_bag = vec_bag.transform(X_val['clean_instruction']) \n",
    "\n",
    "# Результаты\n",
    "print(pd.DataFrame(X_train_bag.toarray(), columns=vec_bag_train.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae9613",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0e65467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        aa  aaa  aabout  aaccount  aailable  aasistance  aat  abailable  abc  \\\n",
      "0      0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "1      0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "2      0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "3      0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "4      0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "...    ...  ...     ...       ...       ...         ...  ...        ...  ...   \n",
      "28251  0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "28252  0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "28253  0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "28254  0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "28255  0.0  0.0     0.0       0.0       0.0         0.0  0.0        0.0  0.0   \n",
      "\n",
      "       abcxyz  ...  yor  yoru  youe  youhave  youhelp  youir  youshow  ypur  \\\n",
      "0         0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "1         0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "2         0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "3         0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "4         0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "...       ...  ...  ...   ...   ...      ...      ...    ...      ...   ...   \n",
      "28251     0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "28252     0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "28253     0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "28254     0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "28255     0.0  ...  0.0   0.0   0.0      0.0      0.0    0.0      0.0   0.0   \n",
      "\n",
      "       yur  zelle  \n",
      "0      0.0    0.0  \n",
      "1      0.0    0.0  \n",
      "2      0.0    0.0  \n",
      "3      0.0    0.0  \n",
      "4      0.0    0.0  \n",
      "...    ...    ...  \n",
      "28251  0.0    0.0  \n",
      "28252  0.0    0.0  \n",
      "28253  0.0    0.0  \n",
      "28254  0.0    0.0  \n",
      "28255  0.0    0.0  \n",
      "\n",
      "[28256 rows x 3420 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train['clean_instruction'])\n",
    "X_val_tfidf = vec_tfidf.transform(X_val['clean_instruction'])\n",
    "\n",
    "# Результаты\n",
    "print(pd.DataFrame(X_train_tfidf.toarray(), columns=vec_tfidf_train.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f378ff5",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b499391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765618, 1170580)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "X_train_tokenized_docs = [word_tokenize(doc) for doc in X_train['clean_instruction']]\n",
    "X_val_tokenized_docs = [word_tokenize(doc) for doc in X_val['clean_instruction']]\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=X_train_tokenized_docs,\n",
    "    vector_size=100,     # размер вектора\n",
    "    window=5,            # размер окна контекста\n",
    "    min_count=1,         # минимальная частота слова\n",
    "    workers=1,           # количество потоков, 1 - иначе рандомит\n",
    "    sg=1                 # 1 для skip-gram, 0 для CBOW\n",
    ")\n",
    "\n",
    "word2vec_model.train(X_train_tokenized_docs, total_examples=len(X_train_tokenized_docs), epochs=10)\n",
    "\n",
    "# взять предобученный w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "77e4744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор для слова 'help':\n",
      "[-0.35163915  0.31268397  0.49166417  0.18095809 -0.4500031  -0.19039911\n",
      "  0.4859719   0.28821427 -0.42653376 -0.56383204 -0.02173221 -0.35532773\n",
      " -0.29702744  0.46302262 -0.09049392  0.10901327  0.00952306 -0.36831054\n",
      " -0.35835877 -0.48667234 -0.02569739 -0.4558875   0.56171143 -0.41802764\n",
      " -0.07442138 -0.12876812 -0.08502872 -0.17657785 -0.20255297  0.01478856\n",
      "  0.37100756 -0.61568016 -0.33489096 -0.6893172   0.02288232  0.18579105\n",
      "  0.80613977  0.52543247  0.00703939 -0.48679838 -0.1365609   0.05675988\n",
      " -0.69686127  0.03633102  0.11787723 -0.11330228 -0.22351696  0.74777347\n",
      "  0.6143787   0.62323964  0.17270592  0.09202094  0.14689662 -0.05326167\n",
      " -0.09115104  0.44239253  0.33902174 -0.09935383 -0.4633681   0.14980492\n",
      "  0.6583257  -0.39091736  0.47928745  0.04092602 -0.02594467  0.6327843\n",
      " -0.5635424   0.3107693  -0.5906118   0.3296252   0.16477898  0.58776623\n",
      " -0.07015681  0.25954866  0.0489395  -0.37317657 -0.4698028   0.24030699\n",
      "  0.11085309 -0.02472962  0.3915914  -0.26958057 -0.24662656 -0.09925059\n",
      "  0.27854806 -0.36611736  0.45534927  0.00505493  0.19297269  0.1115679\n",
      "  0.39031866  0.10245732 -0.286857   -0.06732988  0.8769933   0.26027316\n",
      " -0.4880196  -0.42590815  0.28050256 -0.602286  ]\n",
      "\n",
      "Похожие слова на 'help':\n",
      "[('assistance', 0.8868755102157593), ('uhelp', 0.7966265678405762), ('fucking', 0.7276700735092163), ('asistance', 0.708658754825592), ('goddamn', 0.6982424855232239), ('bloody', 0.6374179124832153), ('try', 0.6364454627037048), ('need', 0.6178781390190125), ('damn', 0.617654025554657), ('ot', 0.6033898591995239)]\n"
     ]
    }
   ],
   "source": [
    "# Получение векторов слов\n",
    "print(\"Вектор для слова 'help':\")\n",
    "print(word2vec_model.wv['help'])\n",
    "\n",
    "# Похожие слова\n",
    "print(\"\\nПохожие слова на 'help':\")\n",
    "print(word2vec_model.wv.most_similar('help', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9340cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина вектора: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Векторное представление документа (среднее векторов слов)\n",
    "def document_vector(model, doc):\n",
    "    words = [word for word in doc if word in model.wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "\n",
    "X_train_w2v = [document_vector(word2vec_model, doc) for doc in X_train_tokenized_docs]\n",
    "X_val_w2v = [document_vector(word2vec_model, doc) for doc in X_val_tokenized_docs]\n",
    "print(f\"Длина вектора: {X_train_w2v[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba9db8",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "137e74c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765259, 1170580)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=1,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "fasttext_model.build_vocab(X_train_tokenized_docs)\n",
    "fasttext_model.train(\n",
    "    X_train_tokenized_docs, \n",
    "    total_examples=len(X_train_tokenized_docs), \n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# взять предобученный ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "228f402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор для существующего слова 'help':\n",
      "[ 1.11393468e-03  4.75587905e-04  1.19153515e-03 -2.63607886e-04\n",
      "  1.30447920e-03  1.99535090e-04 -7.94179796e-04 -1.15824083e-03\n",
      "  5.81545406e-04 -3.27856076e-04 -3.89009423e-04  4.12308174e-04\n",
      " -1.42313109e-03  6.28998620e-04 -5.55658131e-04  2.32535950e-03\n",
      " -1.15962466e-04 -1.49298401e-03 -1.12431333e-03 -1.35366514e-03\n",
      " -9.60248173e-04 -3.45842447e-04 -2.23360606e-03  1.28284283e-03\n",
      " -1.09591370e-03  9.07768437e-04  1.05625973e-03  5.91978314e-04\n",
      " -4.26866136e-05 -2.64778664e-05  9.93702561e-05 -1.89065689e-03\n",
      "  1.66521022e-05  2.12211139e-03  8.87380273e-04 -2.13543564e-04\n",
      " -1.35282008e-03  1.62534649e-04  1.14424278e-04  6.93461916e-04\n",
      " -1.03792537e-03  1.88554055e-03  8.84957670e-04  9.07533686e-04\n",
      "  7.69513543e-04 -4.26217332e-04 -4.96534922e-04 -1.33164623e-03\n",
      " -8.58099142e-04  1.87993236e-03 -3.83349339e-04 -2.88326666e-03\n",
      " -8.69264244e-04  7.40516407e-04 -4.80684230e-06 -1.38817274e-03\n",
      "  9.47783352e-04 -1.19454646e-03  1.10136461e-03 -5.55794802e-04\n",
      " -6.70043111e-04 -4.11323155e-04  3.04796093e-04 -1.11968629e-03\n",
      "  7.50992855e-04 -1.05181767e-03  1.56713161e-03  4.94479260e-04\n",
      " -5.02328738e-04 -5.64250688e-04  2.02150992e-03 -1.64013961e-03\n",
      "  1.10629662e-04 -8.96219513e-04  7.78450107e-04  2.26319116e-03\n",
      " -4.89803904e-04 -1.75448321e-03 -1.76884263e-04 -7.24953657e-04\n",
      "  2.28028119e-04  2.86234717e-04 -1.88233942e-04 -5.01003058e-04\n",
      " -4.94256557e-04 -4.88044490e-04  2.96350248e-04 -1.28054991e-03\n",
      " -4.88924910e-04  1.07543357e-03 -4.09613829e-04 -1.11126236e-03\n",
      "  2.32494203e-03  4.49507934e-04  2.22928647e-04  5.45118353e-04\n",
      "  1.14167528e-03 -1.68255391e-03 -3.38555838e-04  1.69455528e-03]\n",
      "\n",
      "Вектор для OOV слова 'looooooooooooooool':\n",
      "[-0.03704169  0.03283093  0.00117053  0.02328213 -0.01663324  0.00734096\n",
      " -0.01951095  0.00729991  0.07582316 -0.02332984 -0.10064016 -0.04179637\n",
      "  0.0084653   0.04372047 -0.01273832 -0.07725894  0.00513872 -0.07436066\n",
      "  0.00377693 -0.08182042 -0.02729328  0.02040222 -0.08151761 -0.04754639\n",
      " -0.05391644 -0.01588109 -0.01085507  0.03054717  0.05111253 -0.00521682\n",
      " -0.02601693  0.05235725  0.03538332  0.01684639 -0.02937805  0.02775886\n",
      " -0.00619107  0.04359045 -0.06044133  0.04382429 -0.00078043 -0.05525267\n",
      " -0.00607261 -0.08277164 -0.03467165 -0.01068062  0.02713707 -0.02079965\n",
      "  0.01890924  0.01556139  0.04084145  0.01067671  0.02630985  0.05907074\n",
      "  0.00547405  0.00055351 -0.02011814  0.03749527 -0.04326569 -0.04364834\n",
      "  0.04560437 -0.04112291 -0.02702164  0.05307556 -0.02508017  0.05642454\n",
      "  0.00545361 -0.01614899  0.04904567 -0.0033896   0.00806409  0.03442815\n",
      "  0.01129166 -0.02742334 -0.02039738 -0.03444831  0.07671028 -0.00456452\n",
      "  0.02025112  0.00615578  0.0042308   0.00791444  0.03158009 -0.04379914\n",
      "  0.00789059 -0.04628633 -0.05838326  0.03007032 -0.00516992 -0.035278\n",
      " -0.0204171  -0.01565769 -0.01394977 -0.01207721  0.01258264  0.05964865\n",
      "  0.01163375 -0.00418565  0.0399069   0.01118975]\n",
      "\n",
      "Похожие слова на 'help':\n",
      "[('yahelp', 0.9500380158424377), ('helpp', 0.941844642162323), ('helpo', 0.939060389995575), ('youhelp', 0.9278737902641296), ('assistance', 0.9229997992515564), ('uelp', 0.9211400747299194), ('wantassistance', 0.9158444404602051), ('assistancee', 0.9152641296386719), ('assiswtance', 0.9143859148025513), ('assistanceto', 0.9136555194854736)]\n"
     ]
    }
   ],
   "source": [
    "# FastText может работать с OOV (out-of-vocabulary) словами\n",
    "print(\"Вектор для существующего слова 'help':\")\n",
    "print(fasttext_model.wv['машинное'])\n",
    "\n",
    "# Вектор для несуществующего слова (FastText использует n-grams)\n",
    "fake_word = 'looooooooooooooool'\n",
    "print(f\"\\nВектор для OOV слова '{fake_word}':\")\n",
    "print(fasttext_model.wv[fake_word])\n",
    "\n",
    "# Похожие слова\n",
    "print(\"\\nПохожие слова на 'help':\")\n",
    "print(fasttext_model.wv.most_similar('help', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b21dae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина вектора: (100,)\n"
     ]
    }
   ],
   "source": [
    "X_train_ft = [document_vector(fasttext_model, doc) for doc in X_train_tokenized_docs]\n",
    "X_val_ft = [document_vector(fasttext_model, doc) for doc in X_val_tokenized_docs]\n",
    "print(f\"Длина вектора: {X_train_ft[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на подумать - пофиксить орфографию, стоит ли"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f335a",
   "metadata": {},
   "source": [
    "#### Сравнялка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65ab0a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Косинусная схожесть между документами:\n",
      "\n",
      "BoW:\n",
      "[[1.         0.23570226 0.         ... 0.         0.         0.        ]\n",
      " [0.23570226 1.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]]\n",
      "\n",
      "TF-IDF:\n",
      "[[1.         0.13995187 0.         ... 0.         0.         0.        ]\n",
      " [0.13995187 1.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.        ]]\n",
      "\n",
      "Word2Vec:\n",
      "[[0.99999994 0.7913146  0.5672211  ... 0.58703595 0.5994254  0.754567  ]\n",
      " [0.7913146  1.         0.6137414  ... 0.6080403  0.52065533 0.52975214]\n",
      " [0.5672211  0.6137414  1.0000002  ... 0.66772705 0.57719374 0.47496426]\n",
      " ...\n",
      " [0.58703595 0.6080403  0.66772705 ... 1.0000001  0.8519069  0.5922659 ]\n",
      " [0.5994254  0.52065533 0.57719374 ... 0.8519069  1.0000001  0.62406147]\n",
      " [0.754567   0.52975214 0.47496426 ... 0.5922659  0.62406147 1.        ]]\n",
      "\n",
      "FastText:\n",
      "[[1.0000001  0.7733483  0.6297066  ... 0.635419   0.67680424 0.80398   ]\n",
      " [0.7733483  1.0000001  0.648367   ... 0.6571465  0.6017156  0.5651001 ]\n",
      " [0.6297066  0.648367   0.9999999  ... 0.7410618  0.67541033 0.5511861 ]\n",
      " ...\n",
      " [0.635419   0.6571465  0.7410618  ... 1.0000001  0.8430834  0.5565222 ]\n",
      " [0.67680424 0.6017156  0.67541033 ... 0.8430834  0.9999997  0.66579217]\n",
      " [0.80398    0.5651001  0.5511861  ... 0.5565222  0.66579217 0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def compare_methods(docs):\n",
    "#     \"\"\"Сравнение различных методов векторного представления\"\"\"\n",
    "    \n",
    "#     # 1. Bag of Words\n",
    "#     bow_vectorizer = CountVectorizer()\n",
    "#     bow_vectors = bow_vectorizer.fit_transform(docs)\n",
    "    \n",
    "#     # 2. TF-IDF\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     tfidf_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "    \n",
    "#     # 3. Word2Vec\n",
    "#     tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
    "#     w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, sg=1)\n",
    "#     w2v_vectors = np.array([document_vector(w2v_model, doc) for doc in tokenized_docs])\n",
    "    \n",
    "#     # 4. FastText\n",
    "#     ft_model = FastText(vector_size=100, window=5, min_count=1, sg=1)\n",
    "#     ft_model.build_vocab(tokenized_docs)\n",
    "#     ft_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=10)\n",
    "#     ft_vectors = np.array([document_vector(ft_model, doc) for doc in tokenized_docs])\n",
    "    \n",
    "#     # Сравнение косинусной схожести между документами\n",
    "#     methods = {\n",
    "#         'BoW': bow_vectors,\n",
    "#         'TF-IDF': tfidf_vectors,\n",
    "#         'Word2Vec': w2v_vectors,\n",
    "#         'FastText': ft_vectors\n",
    "#     }\n",
    "    \n",
    "#     print(\"Косинусная схожесть между документами:\")\n",
    "#     for method_name, vectors in methods.items():\n",
    "#         if hasattr(vectors, 'toarray'):\n",
    "#             vectors = vectors.toarray()\n",
    "#         similarity = cosine_similarity(vectors)\n",
    "#         print(f\"\\n{method_name}:\")\n",
    "#         print(similarity)\n",
    "\n",
    "# # Запуск сравнения\n",
    "# compare_methods(X_train['clean_instruction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34123976",
   "metadata": {},
   "source": [
    "### Классификация интентов\n",
    "**Модели**\n",
    "\n",
    "Точно пробуем из линейных моделей SVM и логистическую регрессию, но в целом можно поэкспериментировать.\n",
    "\n",
    "Для многоклассовой классификации - можно воспользоваться \"One-vs-Rest\" и \"One-vs-One\"\n",
    "\n",
    "**Метрики** \n",
    "\n",
    "f1-score, precision и recall (micro, macro и weighted - всё имплементировано в sklearn'е)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15780e09",
   "metadata": {},
   "source": [
    "Пара штрихов перед обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d586be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для multilabeled - делаем, чтобы всё хранилось в виде списков\n",
    "\n",
    "y_train['intent'] = y_train['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])\n",
    "y_val['intent'] = y_val['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])\n",
    "y_test['intent'] = y_test['intent'].apply(lambda x: x.split(', ') if isinstance(x, str) else [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d9bbf1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# бинарайзер - для мультилейбл классификатора\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(y_train['intent'])\n",
    "y_val_bin = mlb.transform(y_val['intent'])\n",
    "y_test_bin = mlb.transform(y_test['intent'])\n",
    "\n",
    "y_train_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f60322df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [X_train_bag.toarray(), X_train_tfidf.toarray(), X_train_w2v, X_train_ft]\n",
    "X_val_list = [X_val_bag.toarray(), X_val_tfidf.toarray(), X_val_w2v, X_val_ft]\n",
    "\n",
    "X_train_val_list = [[X_train_bag.toarray(), X_val_bag.toarray()], \n",
    "                    [X_train_tfidf.toarray(), X_val_tfidf.toarray()], \n",
    "                    [X_train_w2v, X_val_w2v], \n",
    "                    [X_train_ft, X_val_ft]\n",
    "                   ]\n",
    "\n",
    "# X_test_list = [X_test_bag.toarray(), X_test_tfidf.toarray(), X_test_w2v, X_test_ft]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd17a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc12b43",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ed3dd7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabeles классификации\n",
    "    # пока сделаем с дефолтными параметрами\n",
    "    model = MultiOutputClassifier(LogisticRegression(random_state=42))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    logreg_list.append(model)\n",
    "    y_pred_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6fa5e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logreg_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabel\n",
    "    # поменяли solver\n",
    "    model = MultiOutputClassifier(LogisticRegression(solver='saga', random_state=42))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    logreg_list.append(model)\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "# обучалось минут 15 :( и всё равно не сошлось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2dc0c298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "Micro F1: 0.945344129554656\n",
      "Macro F1: 0.9449981512441258\n",
      "Weighted F1: 0.9448252212982081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       228\n",
      "           1       0.99      0.93      0.96       183\n",
      "           2       0.97      0.94      0.95       228\n",
      "           3       1.00      0.96      0.98       226\n",
      "           4       0.94      0.89      0.92       246\n",
      "           5       0.99      0.94      0.96       231\n",
      "           6       0.98      0.91      0.95       257\n",
      "           7       1.00      0.89      0.94       222\n",
      "           8       0.98      0.91      0.94       205\n",
      "           9       0.94      0.94      0.94       254\n",
      "          10       0.96      0.90      0.93       226\n",
      "          11       0.99      0.96      0.97       263\n",
      "          12       0.96      0.86      0.91       249\n",
      "          13       0.98      0.92      0.95       216\n",
      "          14       0.97      0.87      0.92       206\n",
      "          15       0.95      0.85      0.90       230\n",
      "          16       0.95      0.90      0.92       229\n",
      "          17       1.00      0.99      0.99       223\n",
      "          18       0.98      0.96      0.97       224\n",
      "          19       0.98      0.80      0.88       235\n",
      "          20       1.00      0.97      0.98       227\n",
      "          21       0.99      0.92      0.95       220\n",
      "          22       0.99      0.98      0.99       182\n",
      "          23       0.99      0.96      0.98       227\n",
      "          24       0.95      0.93      0.94       218\n",
      "          25       0.96      0.83      0.89       199\n",
      "          26       0.98      0.96      0.97       254\n",
      "\n",
      "   micro avg       0.97      0.92      0.95      6108\n",
      "   macro avg       0.98      0.92      0.94      6108\n",
      "weighted avg       0.97      0.92      0.94      6108\n",
      " samples avg       0.92      0.92      0.92      6108\n",
      "\n",
      "----------\n",
      "TF-IDF\n",
      "Micro F1: 0.915005608766934\n",
      "Macro F1: 0.9136589475878044\n",
      "Weighted F1: 0.9139970852670557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       228\n",
      "           1       0.90      0.82      0.86       183\n",
      "           2       0.97      0.86      0.91       228\n",
      "           3       1.00      0.95      0.97       226\n",
      "           4       0.93      0.81      0.87       246\n",
      "           5       0.98      0.90      0.94       231\n",
      "           6       0.97      0.87      0.92       257\n",
      "           7       1.00      0.88      0.94       222\n",
      "           8       0.99      0.87      0.92       205\n",
      "           9       0.94      0.92      0.93       254\n",
      "          10       0.96      0.85      0.90       226\n",
      "          11       0.99      0.92      0.95       263\n",
      "          12       0.96      0.82      0.89       249\n",
      "          13       0.97      0.87      0.92       216\n",
      "          14       0.96      0.79      0.87       206\n",
      "          15       0.95      0.79      0.86       230\n",
      "          16       0.95      0.80      0.87       229\n",
      "          17       1.00      0.95      0.97       223\n",
      "          18       0.99      0.94      0.96       224\n",
      "          19       0.98      0.72      0.83       235\n",
      "          20       0.99      0.95      0.97       227\n",
      "          21       0.99      0.85      0.91       220\n",
      "          22       0.99      0.97      0.98       182\n",
      "          23       0.99      0.95      0.97       227\n",
      "          24       0.96      0.84      0.90       218\n",
      "          25       0.92      0.78      0.84       199\n",
      "          26       0.96      0.91      0.94       254\n",
      "\n",
      "   micro avg       0.97      0.87      0.92      6108\n",
      "   macro avg       0.97      0.87      0.91      6108\n",
      "weighted avg       0.97      0.87      0.91      6108\n",
      " samples avg       0.87      0.87      0.87      6108\n",
      "\n",
      "----------\n",
      "W2V\n",
      "Micro F1: 0.9093367131389946\n",
      "Macro F1: 0.9082356797069785\n",
      "Weighted F1: 0.9083535865006989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91       228\n",
      "           1       0.86      0.80      0.83       183\n",
      "           2       0.95      0.83      0.89       228\n",
      "           3       0.99      0.99      0.99       226\n",
      "           4       0.89      0.79      0.84       246\n",
      "           5       0.97      0.96      0.97       231\n",
      "           6       0.93      0.88      0.91       257\n",
      "           7       0.99      0.91      0.95       222\n",
      "           8       0.97      0.88      0.93       205\n",
      "           9       0.97      0.95      0.96       254\n",
      "          10       0.92      0.89      0.91       226\n",
      "          11       0.95      0.88      0.91       263\n",
      "          12       0.92      0.82      0.87       249\n",
      "          13       0.95      0.88      0.91       216\n",
      "          14       0.91      0.80      0.85       206\n",
      "          15       0.89      0.82      0.85       230\n",
      "          16       0.89      0.81      0.85       229\n",
      "          17       1.00      0.97      0.98       223\n",
      "          18       0.95      0.93      0.94       224\n",
      "          19       0.88      0.74      0.81       235\n",
      "          20       0.99      0.99      0.99       227\n",
      "          21       0.96      0.90      0.93       220\n",
      "          22       0.98      0.99      0.99       182\n",
      "          23       0.95      0.95      0.95       227\n",
      "          24       0.92      0.85      0.88       218\n",
      "          25       0.89      0.79      0.84       199\n",
      "          26       0.93      0.89      0.91       254\n",
      "\n",
      "   micro avg       0.94      0.88      0.91      6108\n",
      "   macro avg       0.94      0.88      0.91      6108\n",
      "weighted avg       0.94      0.88      0.91      6108\n",
      " samples avg       0.87      0.89      0.88      6108\n",
      "\n",
      "----------\n",
      "FT\n",
      "Micro F1: 0.9050487908358082\n",
      "Macro F1: 0.9043150925813194\n",
      "Weighted F1: 0.903955641334525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       228\n",
      "           1       0.90      0.78      0.83       183\n",
      "           2       0.95      0.80      0.87       228\n",
      "           3       0.99      0.98      0.99       226\n",
      "           4       0.91      0.80      0.85       246\n",
      "           5       0.98      0.95      0.96       231\n",
      "           6       0.92      0.86      0.89       257\n",
      "           7       0.99      0.91      0.95       222\n",
      "           8       0.97      0.89      0.93       205\n",
      "           9       0.96      0.95      0.95       254\n",
      "          10       0.92      0.88      0.90       226\n",
      "          11       0.94      0.84      0.89       263\n",
      "          12       0.89      0.81      0.85       249\n",
      "          13       0.94      0.81      0.87       216\n",
      "          14       0.93      0.80      0.86       206\n",
      "          15       0.91      0.83      0.87       230\n",
      "          16       0.87      0.78      0.82       229\n",
      "          17       0.99      0.97      0.98       223\n",
      "          18       0.94      0.91      0.93       224\n",
      "          19       0.88      0.74      0.80       235\n",
      "          20       0.99      0.99      0.99       227\n",
      "          21       0.96      0.89      0.92       220\n",
      "          22       0.98      0.98      0.98       182\n",
      "          23       0.97      0.94      0.95       227\n",
      "          24       0.91      0.88      0.90       218\n",
      "          25       0.91      0.81      0.86       199\n",
      "          26       0.90      0.87      0.89       254\n",
      "\n",
      "   micro avg       0.94      0.87      0.91      6108\n",
      "   macro avg       0.94      0.87      0.90      6108\n",
      "weighted avg       0.94      0.87      0.90      6108\n",
      " samples avg       0.86      0.88      0.87      6108\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "vecs = ['BoW', 'TF-IDF', 'W2V', 'FT']\n",
    "for i in range(4):\n",
    "    print(vecs[i])\n",
    "    print(\"Micro F1:\", f1_score(y_val_bin, y_pred_list[i], average='micro'))\n",
    "    print(\"Macro F1:\", f1_score(y_val_bin, y_pred_list[i], average='macro')) \n",
    "    print(\"Weighted F1:\", f1_score(y_val_bin, y_pred_list[i], average='weighted'))\n",
    "    print(classification_report(y_val_bin, y_pred_list[i]))\n",
    "    print('-'*10)\n",
    "\n",
    "    \n",
    "# модель предсказала все нули???\n",
    "# стоит ли сделать , zero_division=0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ff8bf",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "def6261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "\n",
    "svc_list = []\n",
    "y_svc_pred_list = []\n",
    "\n",
    "for x_train_val in X_train_val_list:\n",
    "    # MultiOutputClassifier - обёртка для multilabel классификации\n",
    "    # поменяли solver\n",
    "    model = MultiOutputClassifier(LinearSVC(random_state=42))\n",
    "    model.fit(x_train_val[0], y_train_bin)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = model.predict(x_train_val[1])\n",
    "    \n",
    "    # Сохранили модель и предикт\n",
    "    svc_list.append(model)\n",
    "    y_svc_pred_list.append(y_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2f12e13f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW\n",
      "Micro F1: 0.9628712871287128\n",
      "Macro F1: 0.9627959044823583\n",
      "Weighted F1: 0.9627172534891602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       228\n",
      "           1       0.97      0.95      0.96       183\n",
      "           2       0.97      0.96      0.96       228\n",
      "           3       0.99      0.99      0.99       226\n",
      "           4       0.93      0.93      0.93       246\n",
      "           5       0.99      0.98      0.98       231\n",
      "           6       0.98      0.94      0.96       257\n",
      "           7       1.00      0.95      0.97       222\n",
      "           8       0.96      0.91      0.93       205\n",
      "           9       0.93      0.94      0.94       254\n",
      "          10       0.96      0.95      0.96       226\n",
      "          11       0.99      0.98      0.99       263\n",
      "          12       0.93      0.95      0.94       249\n",
      "          13       0.97      0.95      0.96       216\n",
      "          14       0.96      0.95      0.95       206\n",
      "          15       0.96      0.89      0.93       230\n",
      "          16       0.95      0.93      0.94       229\n",
      "          17       1.00      1.00      1.00       223\n",
      "          18       0.99      0.98      0.98       224\n",
      "          19       0.96      0.88      0.92       235\n",
      "          20       1.00      1.00      1.00       227\n",
      "          21       0.96      0.97      0.97       220\n",
      "          22       0.99      0.99      0.99       182\n",
      "          23       0.98      0.98      0.98       227\n",
      "          24       0.97      0.97      0.97       218\n",
      "          25       0.95      0.91      0.93       199\n",
      "          26       0.98      0.99      0.98       254\n",
      "\n",
      "   micro avg       0.97      0.96      0.96      6108\n",
      "   macro avg       0.97      0.96      0.96      6108\n",
      "weighted avg       0.97      0.96      0.96      6108\n",
      " samples avg       0.95      0.96      0.95      6108\n",
      "\n",
      "----------\n",
      "TF-IDF\n",
      "Micro F1: 0.9569457069663664\n",
      "Macro F1: 0.9567661969197987\n",
      "Weighted F1: 0.9567424299727111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       228\n",
      "           1       0.97      0.93      0.95       183\n",
      "           2       0.96      0.93      0.94       228\n",
      "           3       1.00      0.99      0.99       226\n",
      "           4       0.93      0.90      0.92       246\n",
      "           5       0.99      0.97      0.98       231\n",
      "           6       0.98      0.92      0.95       257\n",
      "           7       1.00      0.93      0.97       222\n",
      "           8       0.96      0.90      0.93       205\n",
      "           9       0.93      0.96      0.95       254\n",
      "          10       0.95      0.96      0.95       226\n",
      "          11       0.98      0.98      0.98       263\n",
      "          12       0.94      0.94      0.94       249\n",
      "          13       0.97      0.94      0.96       216\n",
      "          14       0.96      0.90      0.93       206\n",
      "          15       0.93      0.89      0.91       230\n",
      "          16       0.94      0.93      0.94       229\n",
      "          17       1.00      0.99      1.00       223\n",
      "          18       0.97      0.98      0.97       224\n",
      "          19       0.97      0.88      0.92       235\n",
      "          20       1.00      1.00      1.00       227\n",
      "          21       0.97      0.97      0.97       220\n",
      "          22       0.99      1.00      1.00       182\n",
      "          23       0.97      0.99      0.98       227\n",
      "          24       0.96      0.95      0.96       218\n",
      "          25       0.92      0.90      0.91       199\n",
      "          26       0.96      0.97      0.97       254\n",
      "\n",
      "   micro avg       0.97      0.95      0.96      6108\n",
      "   macro avg       0.97      0.95      0.96      6108\n",
      "weighted avg       0.97      0.95      0.96      6108\n",
      " samples avg       0.94      0.95      0.95      6108\n",
      "\n",
      "----------\n",
      "W2V\n",
      "Micro F1: 0.9260187040748162\n",
      "Macro F1: 0.9258495925902431\n",
      "Weighted F1: 0.925449936141268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       228\n",
      "           1       0.93      0.89      0.91       183\n",
      "           2       0.94      0.86      0.90       228\n",
      "           3       0.99      1.00      1.00       226\n",
      "           4       0.91      0.82      0.86       246\n",
      "           5       0.99      0.97      0.98       231\n",
      "           6       0.95      0.89      0.92       257\n",
      "           7       0.99      0.92      0.96       222\n",
      "           8       0.97      0.90      0.93       205\n",
      "           9       0.96      0.95      0.95       254\n",
      "          10       0.92      0.91      0.92       226\n",
      "          11       0.96      0.92      0.94       263\n",
      "          12       0.91      0.84      0.87       249\n",
      "          13       0.95      0.88      0.91       216\n",
      "          14       0.93      0.84      0.88       206\n",
      "          15       0.88      0.84      0.86       230\n",
      "          16       0.89      0.86      0.88       229\n",
      "          17       0.99      0.99      0.99       223\n",
      "          18       0.96      0.96      0.96       224\n",
      "          19       0.92      0.81      0.86       235\n",
      "          20       1.00      0.99      0.99       227\n",
      "          21       0.95      0.93      0.94       220\n",
      "          22       0.99      0.99      0.99       182\n",
      "          23       0.96      0.96      0.96       227\n",
      "          24       0.91      0.88      0.90       218\n",
      "          25       0.90      0.86      0.88       199\n",
      "          26       0.92      0.94      0.93       254\n",
      "\n",
      "   micro avg       0.94      0.91      0.93      6108\n",
      "   macro avg       0.94      0.91      0.93      6108\n",
      "weighted avg       0.94      0.91      0.93      6108\n",
      " samples avg       0.90      0.91      0.90      6108\n",
      "\n",
      "----------\n",
      "FT\n",
      "Micro F1: 0.9197129026873644\n",
      "Macro F1: 0.9192900224030988\n",
      "Weighted F1: 0.9190729313772507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       228\n",
      "           1       0.91      0.84      0.87       183\n",
      "           2       0.95      0.86      0.90       228\n",
      "           3       0.99      0.99      0.99       226\n",
      "           4       0.91      0.83      0.87       246\n",
      "           5       0.98      0.97      0.98       231\n",
      "           6       0.93      0.88      0.90       257\n",
      "           7       0.98      0.94      0.96       222\n",
      "           8       0.96      0.90      0.93       205\n",
      "           9       0.95      0.96      0.96       254\n",
      "          10       0.90      0.93      0.92       226\n",
      "          11       0.93      0.90      0.92       263\n",
      "          12       0.92      0.83      0.87       249\n",
      "          13       0.94      0.87      0.90       216\n",
      "          14       0.92      0.83      0.88       206\n",
      "          15       0.89      0.85      0.87       230\n",
      "          16       0.88      0.82      0.85       229\n",
      "          17       0.98      0.98      0.98       223\n",
      "          18       0.94      0.94      0.94       224\n",
      "          19       0.87      0.80      0.83       235\n",
      "          20       0.99      0.99      0.99       227\n",
      "          21       0.96      0.91      0.93       220\n",
      "          22       0.99      0.99      0.99       182\n",
      "          23       0.96      0.95      0.95       227\n",
      "          24       0.91      0.90      0.91       218\n",
      "          25       0.90      0.83      0.86       199\n",
      "          26       0.93      0.92      0.93       254\n",
      "\n",
      "   micro avg       0.94      0.90      0.92      6108\n",
      "   macro avg       0.94      0.90      0.92      6108\n",
      "weighted avg       0.94      0.90      0.92      6108\n",
      " samples avg       0.89      0.91      0.90      6108\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vadim/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "vecs = ['BoW', 'TF-IDF', 'W2V', 'FT']\n",
    "for i in range(4):\n",
    "    print(vecs[i])\n",
    "    print(\"Micro F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='micro'))\n",
    "    print(\"Macro F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='macro')) \n",
    "    print(\"Weighted F1:\", f1_score(y_val_bin, y_svc_pred_list[i], average='weighted'))\n",
    "    print(classification_report(y_val_bin, y_svc_pred_list[i]))\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a86ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd802ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162065b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad18d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e373c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8846ac",
   "metadata": {},
   "source": [
    "## Вопросы на подумать\n",
    "- **подозрительно хороший скор** - кажется, я где-то жестоко неправ\n",
    "- мультикласс - в нашем случае лучше onehotencode'ить каждую вариацию? или рассматривать комбинацию интентов как отдельный класс? unique-значения и играться порогами для моделей, чтобы было 1-2 метки предсказанных\n",
    "- как бы чистить выбросы типо \"question abobut, aaaaabout\" и прочее? или их не надо чистить, считаем это нормальным шумом, который вполне реален? не теряет ли модель качество в этом шуме?\n",
    "- ~~надо преобразование для трейна и валидации делать одной или разными векторайзерами? по логике кажется, что одним~~\n",
    "- **пороги вероятностей для моделей**\n",
    "\n",
    "### Мысли\n",
    "- не сходится логрег и оч долго обучается - что-то странное\n",
    "- точно нужно попробовать подбор гиперпараметров, с помощью GridSearchCV к примеру\n",
    "- модель предсказала все нули??? стоит ли сделать , zero_division=0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980d774",
   "metadata": {},
   "source": [
    "- думаем про стоп-слова и комменты повыше\n",
    "- можно подумать над доп. метриками (взвешенный accuracy, кастомные)\n",
    "- перебераем параметры, смотрим на метрики, мб графики\n",
    "- попробовать ещё моделей\n",
    "- пайплайн обучения, пайплайн применения - всё объединить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cee26b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 27)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 0][0], \n",
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "26077919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 27)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "0e81ccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 27)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 0][0], \n",
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4bd253f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 27)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_svc_pred_list[0][np.sum(y_svc_pred_list[0], axis=1) == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "83830184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0][np.sum(y_pred_list[0], axis=1) >= 2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8f6987ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409498d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
